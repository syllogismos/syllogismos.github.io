
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Facebook Link Prediction - My Blog</title>
  <meta name="author" content="syllogismos">

  
  <meta name="description" content="note: I did this just as an exercise, you get much more from this post. Link Prediction: We are given snapshot of a network and would like to infer &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://syllogismos.github.io/blog/2014/08/10/facebook-link-prediction/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="My Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">My Blog</a></h1>
  
    <h2>My learnings and etc.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="syllogismos.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Facebook Link Prediction</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-08-10T08:11:37+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>8:11 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>note: I did this just as an exercise, you get much more from <a href="http://blog.echen.me/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">this post</a>.</p>

<h2>Link Prediction:</h2>

<p>We are given snapshot of a network and would like to infer which which interactions among existing
members are likely to occur in the near future or which existing interactions are we missing. The challenge is to effectively combine the information from the network structure with rich node and edge
attribute data.</p>

<h2>Supervised Random Walks:</h2>

<p>This repository is the implementation of Link prediction based on the paper Supervised Random Walks by  Lars Backstrom et al. The essence of which is that we combine the information from the network structure with the node and edge level attributes using Supervised Random Walks. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function.</p>

<h2>Problem Description:</h2>

<p>We are given a directed graph <em>G(V,E)</em>, a node <em>s</em> and a set of candidates to which <em>s</em> could create an edge. We label nodes to which <em>s</em> creates edges in the future as <em>destination nodes D = {d<sub>1</sub>,..,d<sub>k</sub>}</em>, while we call the other nodes to which s does not create edges no-link nodes <em>L = {l<sub>1</sub>,..,l<sub>n</sub>}</em>. We label candidate nodes with a set <em>C = D union L</em>. <em>D</em> are positive training examples and <em>L</em> are negative training examples. We can generalize this to multiple instances of <em>s, D, L</em>. Each node and each edge in G is further described with a set of features. We assume that each edge <em>(u,v)</em> has a corresponding feature vector psi<sub>uv</sub> that describes u and v and the interaction attributes.</p>

<p>For each edge (u,v) in G we compute the strength <em>a<sub>uv</sub> = f<sub>w</sub>(psi<sub>uv</sub>)</em>. Function <em>f<sub>w</sub></em> parameterized by <em>w</em> takes the edge feature vector <em>psi<sub>uv</sub></em> as input and computes the corresponding edge strength <em>a<sub>uv</sub></em> that models the random walk transition probability. It is exactly the function <em>f<sub>w</sub>(psi)</em> we learn in the training phase of the algorithm.</p>

<p>To predict new edges to <em>s</em>, first edge strengths of all edges are calculated using <em>f<sub>w</sub></em>. Then random walk with restarts is run from <em>s</em>. The stationary distribution <em>p</em> of random walk assigns each node <em>u</em> a probability <em>p<sub>u</sub></em>. Nodes are ordered by <em>p<sub>u</sub></em> and top ranked nodes are predicted as future destination nodes to <em>s</em>. The task is to learn the parameters <em>w</em> of function <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> that assigns each edge a transition probability. One can think of the weights <em>a<sub>uv</sub></em> as edge strengths and the random walk is more likely to traverse edges of high strength and thus nodes connected to node <em>s</em> via paths of strong edges will likely to be visited by the random walk and will thus rank higher.</p>

<h3>The optimization problem:</h3>

<p>The training data contains information that source node <em>s</em> will create edges to node <em>d subset D</em> and not <em>l subset L</em>. So we set parameters <em>w</em> of the function <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> so that it will assign edge weights <em>a<sub>uv</sub></em> in such a way that the random walk will be more likely to visit nodes in <em>D</em> than <em>L</em>, <em>i.e.,</em> <em>p<sub>l</sub> &lt; p<sub>d</sub></em> for each <em>d subset D</em> and <em>l subset L</em>. And thus we define the optimization problem as follows.<br/>
<img src="http://i.imgur.com/zMjJ1Nb.png" alt="optimization problem hard version" /></p>

<p>where <em>p</em> is the vector of pagerank scores. Pagerank scores <em>p<sub>i</sub></em> depend on edge strength on <em>a<sub>uv</sub></em> and thus actually depend on <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> which is parameterized by <em>w</em>. The above equation (1) simply states that we want to find <em>w</em> such that the pagerank score of nodes in <em>D</em> will be greater than the scores of nodes in <em>L</em>. We prefer the shortest <em>w</em> parameters simply for the sake of regularization. But the above equation is the &ldquo;hard&rdquo; version of the optimization problem. However it is unlikely that a solution satisfying all the constraints exist. We make the optimization problem &ldquo;soft&rdquo; by introducing a loss function that penalizes the violated constraints. Now the optimization problem becomes,<br/>
<img src="http://i.imgur.com/oZ2pYN1.png" alt="optimization problem soft version." /><br/>
where lambda is the regularization parameter that trades off between the complexity(norm of <em>w</em>) for the fit of the model(how much the constraints can be violated). And <em>h(.)</em> is a loss function that assigns a non-negative penalty according to the difference of the scores <em>p<sub>l</sub>-p<sub>d</sub></em>. <em>h(.) = 0</em> if <em>p<sub>l</sub> &lt; p<sub>d</sub></em> as the constraint is not violated and <em>h(.) > 0</em> if <em>p<sub>l</sub> > p<sub>d</sub></em></p>

<h3>Solving the optimization problem:</h3>

<p>First we need to establish connection between the parameters <em>w</em> and the random walk scores <em>p</em>. Then we show how to obtain partial derivatives of the loss function and <em>p</em> with respect to <em>w</em> and then perform gradient descent to obtain optimal values of <em>w</em> and minimize loss.
We build a random walk stochastic transition matrix <em>Q<sup>&lsquo;</sup></em> from the edge strengths <em>a<sub>uv</sub></em> calculated from <em>f<sub>w</sub>(psi<sub>uv</sub>)</em>.<br/>
<img src="http://i.imgur.com/JiSHf7t.png" alt="Q dash" /></p>

<p>To obtain the final random walk transition probability matrix <em>Q</em>, we also incorporate the restart probability <em>alpha</em>, <em>i.e.,</em> the probability with which the random walk jumps back to seed node <em>s</em>, and thus &ldquo;restarts&rdquo;.<br/>
<img src="http://i.imgur.com/vE2P7LJ.png" alt="Q" /></p>

<p>each entry <em>Q<sub>uv</sub></em> deÔ¨Ånes the conditional probability that a walk will traverse edge (u, v) given that it is currently at node u.
The vector <em>p</em> is the stationary distribution of the Random Walk with restarts(also known as Personalized Page Rank), and is the solution to the following eigen vector equation.<br/>
<img src="http://i.imgur.com/UFwnobA.png" alt="eigen vector equation" /></p>

<p>The above equation establishes the connection between page rank scores <em>p</em> and the parameters <em>w</em> via the random walk transition probability matrix <em>Q</em>. Our goal now is to minimize the soft version of the loss function(eq. 2) with respect to parameter vector <em>w</em>. We do this by obtaining the gradient of <em>F(w)</em> with respect to <em>w</em>, and then performing gradient based optimization method to find <em>w</em> that minimize <em>F(w)</em>. This gets complicated due to the fact that equation 4 is recursive. For this we introduce <em>delta<sub>ld</sub> = p<sub>l</sub>-p<sub>d</sub></em> and then we can write the derivative<br/>
<img src="http://i.imgur.com/FhZVZEB.png" alt="delta ld" /><br/>
and then we can write the derivative of <em>F(w)</em> as follows<br/>
<img src="http://i.imgur.com/oisE40X.png" alt="lossfunction gradient with delta" /><br/>
For commonly used loss functions <em>h(.)</em> it is easy to calculate derivative, but it is not clear how to obtain partial derivatives of <em>p</em> wrt <em>w</em>. <em>p</em> is the principle eigen vector of matrix <em>Q</em>. The above eigen vector equation can also be written as follows.<br/>
<img src="http://i.imgur.com/z00CXm4.png" alt="eigen vector reduced form." /><br/>
and taking the derivatives now gives<br/>
<img src="http://i.imgur.com/FhZVZEB.png" alt="derivative of p recursive form" /><br/>
above <em>p<sub>u</sub></em> and its partial derivative are entangled in the equation, however we compute the above values iteratively as follows<br/>
<img src="http://i.imgur.com/WneoOOn.png" alt="power method to compute p and its partial derivative iteratively." /><br/>
we initialize the vector <em>p</em> as <em>1/|V|</em> and all its derivatives as zeroes before the iteration starts and terminates the recursion till the <em>p</em> and its derivatives converge for an epsilon say <em>10e-12</em>.
To solve equation 4, we need partial derivative of <em>Q<sub>ju</sub></em>, this calculation is straight forward. When <em>(j,u) subset E</em> derivative of <em>Q<sub>ju</sub></em> is<br/>
<img src="http://i.imgur.com/aLDlWP4.png" alt="partial derivative of Qju" /><br/>
and derivative of <em>Q<sub>ju</sub></em> is zero if edge <em>(j,u)</em> is not a subset of <em>E</em>.</p>

<h2>My Implementation:</h2>

<p>We are given a huge network with existing connections. When predicting future link of a particular node, we consider that <em>s</em>, and the graph <em>G(E,V)</em> is
Here we explain how each helper function and main functions implements the above algorithm..</p>

<h3>FeaturesFromAdjacentMatrix.m:</h3>

<p>This is a temporary function specific to the facebook data that generates Features of each edge from a given adjacency matrix. For other problems this function must be replaced with something that generates feature vector for each edge based on graph <em>G(E,V)</em> and node, edge attributes. For an network with <em>n</em> nodes this function returns <em>n x n x m</em> matrix, where <em>m</em> is the size of parameter vector <em>w</em>(sometimes <em>m+1</em>)</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>Adjacency matrix, node attributes, edge attributes</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>psi size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>FeaturesToEdgeStrength.m:</h3>

<p>This function takes the feature matrix (<em>psi</em>) and the parameter vector (<em>w</em>) as arguments to return edge strength (<em>A</em>) and partial derivative of edge strength wrt to each parameter(<em>dA</em>). We also compute partial derivative of edge strength to make further calculations easier. We can vary edge strength function in future implementations, in this we used <em>sigmod(w x psi<sub>uv</sub>)</em> as edge strength function.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>psi size(nxnxm)</em></li>
<li><em>w size(1xm)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>dA size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>EdgeStrengthToTransitionProbability.m</h3>

<p>This function takes the edge strength matrix <em>A</em> and <em>alpha</em> to compute transition probability matrix <em>Q</em>.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>alpha size(1x1)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>Q size(nxn)</em></li>
</ul>
</li>
</ul>


<h3>EdgeStrengthToPartialdiffTransition.m</h3>

<p>This function computes partial derivative of transition probability matrix from <em>A</em>, <em>dA</em> and <em>alpha</em></p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>dA size(nxnxm)</em></li>
<li><em>alpha size(1x1)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>dQ size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>LossFunction.m</h3>

<p>This function takes as input parameters, adjacency matrix of the network, lambda and alpha.
* We get edge strength matrix and its partial derivatives from features and parameters
* We get transition probability and partial derivatives of it from <em>A</em> and <em>dA</em>
* We get stationary probabilities from <em>Q</em> and <em>dQ</em>
* Compute cost and gradient from the above variables, we can use various functions as loss function <em>h(.)</em>. Here we used wilcoxon loss function.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>param: parameters of the edge strength function, size(1,m)</li>
<li>features: features of all the edges in the network, size(n,n,m)</li>
<li>d: binary vector representing destination nodes, size(1,n)</li>
<li>lambda: regularization parameter, size(1,1)</li>
<li>alpha: random restart parameter, size(1,1)</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li>J: loss, size(1,1)</li>
<li>grad: gradient of cost wrt parameters, size(1,m)</li>
</ul>
</li>
</ul>


<h3>fmincg.m</h3>

<p>We use this function to do the minimization of the loss function, given a starting point for the parameters, and the function that computes loss and gradients for a given parameter vector. This is similar to fminunc function available in octave.</p>

<h3>GetNodesFromParam.m</h3>

<p>This function calculates the closest nodes to the root node given the parameters obtained after training.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>param: parameters, size(m,1)</li>
<li>features: feature matrix, size(n,n,m)</li>
<li>d: binary vector representing the destination nodes, size(1,n)</li>
<li>alpha: alpha value used in calculation of Q, size(1,1)</li>
<li>y: number of nodes to output</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li>nodes: output nodes, size(1,y)</li>
<li>P: probabilities of the nodes, size(1,n)</li>
</ul>
</li>
</ul>


<h2>How to Train:</h2>

<p>Here I will show how to train the supervised random walk for a given root node <em>s</em> and edge features matrix <em>psi</em>.
I&rsquo;m not showing how to obtain the edge features. Given the network structure, node and edge attributes etc, you can experiment with different feature extraction techniques.
Here we have <em>psi</em></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>octave:1&gt; clear, close all, clc;
</span><span class='line'>octave:2&gt; rand("seed", 3410);
</span><span class='line'>octave:3&gt; m = size(psi)(3);
</span><span class='line'>octave:4&gt; n = size(psi)(1);
</span><span class='line'>octave:5&gt; initial_w = zeroes(1,m);   % initialize the parameters to zeros or rand
</span><span class='line'>octave:6&gt; initial_w = rand(1,n);
</span><span class='line'>
</span><span class='line'>% to calculate the loss for a given parameter vector.
</span><span class='line'>
</span><span class='line'>octave:7&gt; [loss, grad] = LossFunction(initial_w, psi, d, lambda=1, alpha=0.2, b=0.4);
</span><span class='line'>
</span><span class='line'>% d above is a binary vector that represents the destination nodes to begin with, 
</span><span class='line'>% you can initialize this randomly or obtain it from the graph
</span><span class='line'>
</span><span class='line'>% training
</span><span class='line'>octave:8&gt; options = optimset('GradObj', 'on', 'MaxIter', 20);
</span><span class='line'>octave:9&gt; [w,loss] = ...
</span><span class='line'>  fmincg(@(t)(LossFunction(t, psi, d, lambda=1,alpha=0.2,b=0.4)),
</span><span class='line'>  initial_w, options);
</span><span class='line'>
</span><span class='line'>% w obtained above is the parameters we obtained after gradient descent
</span><span class='line'>
</span><span class='line'>octave:10&gt; y = 10;
</span><span class='line'>octave:11&gt; [nodes, P] = GetNodesFromParam(w, psi,d,alpha = 0.2,y);</span></code></pre></td></tr></table></div></figure>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">syllogismos</span></span>

      




<time class='entry-date' datetime='2014-08-10T08:11:37+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>8:11 am</span></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://syllogismos.github.io/blog/2014/08/10/facebook-link-prediction/" data-via="2abstract4me" data-counturl="http://syllogismos.github.io/blog/2014/08/10/facebook-link-prediction/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/08/10/welcome-to-my-website/" title="Previous Post: Welcome to my Blog">&laquo; Welcome to my Blog</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/08/10/octopress-blog-as-user-page-in-github/" title="Next Post: Octopress blog as User Page in Github, using Windows">Octopress blog as User Page in Github, using Windows &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/10/04/learning-to-walk/">Learning to Walk</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/08/02/miscellanious-notes/">Miscellanious Notes</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/07/30/elasticsearch-segmentation-moengage/">Elasticsearch, Segmentation, MoEngage</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/02/santander-product-recommendation-kaggle/">Santander Product Recommendation Kaggle</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/15/ghost-blog-as-your-github-profile-page/">Ghost Blog as Your Github User Page</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/syllogismos">@syllogismos</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'syllogismos',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - syllogismos -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
