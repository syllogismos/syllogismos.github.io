
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Elasticsearch, Segmentation, MoEngage - My Blog</title>
  <meta name="author" content="syllogismos">

  
  <meta name="description" content="For almost two years I worked at a companay called MoEngage, which is a marketing automation b2b company for app developers. We handle push messages &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://syllogismos.github.io/blog/2017/07/30/elasticsearch-segmentation-moengage/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="My Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">My Blog</a></h1>
  
    <h2>My learnings and etc.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="syllogismos.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Elasticsearch, Segmentation, MoEngage</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-30T12:11:45+05:30'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>12:11 pm</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>For almost two years I worked at a companay called <a href="https://moengage.com/">MoEngage</a>, which is a marketing automation b2b company for app developers. We handle push messages, inapp messages for both mobile phone apps and web apps, email campaigns etc,. I was one of the early employees and I worked on <strong>Segmentation</strong> and built it from ground up. The early days were probably my favourite days, everyone worked with so much intensity at a very fast pace with no scope for any distractions. Very productive and rewarding work. My work touched almost all aspects of the buisness and the entire tech stack we were using. I mainly want this to be a tech blog post about the implementation of <strong>segmentation</strong> using <strong>elasticsearch</strong> but also share some learning experiences being part of a budding young startup with no prior experience of building an engineering team., and I regret so much not writing this earlier, now that it&rsquo;s been a while me quitting that job, I definitely wont be able to be as comprehensive as I would like it to be. I was very comfortable with the elasticsearch, celery workers, redis, s3, sqs, aws, python debugging, release process, testing at scale etc., I faced and solved very intersting and unique challenges specific to our use case. Please forgive me for any grammatical or spelling mistakes. I postponed this post for the single reason of wanting to do it perfectly, but the exact opposite is happening. Now I just can&rsquo;t wait anymore to get this out.</p>

<h1>Problem statement and context.</h1>

<p>As I said, briefly Moengage is a marketing automation solution for apps, using moengage, apps can send push messages to users of an app and target a very specific  segment of users and reduce churn. At that point of time the mvp of moengage is that we can target users very specifically based on their behaviour inside the app or their own characteristics like their age/sex/location and other attributes. But none of that is built yet. And the few clients that were using us were just bulk messaging(<em>spamming</em>) all the users with the same push message. Just get a cursor on the user database, iterate through them all and send each user to the push worker and finally send the push messages. Turns out even this is hard for apps to do, they would rather use some 3rd party to do handle the push infrastructure and they just have a nice dashboard where their marking folk can put in what push message to send, like offers, deals and other stuff.</p>

<p>We provide sdk to the app developers where we give them two main end points. The sdk does a lot more than this, but for this blog post the only endpoints that we are concerend with are the below two</p>

<ul>
<li><p>One is to track <strong>Users</strong>. This endpoint can send stuff like name, location, city, sex, age, email. Everything that can be considered as a feature of the user. We call these features <strong><em>User attributes</em></strong></p></li>
<li><p>And another is to track what the user is doing inside the app we call these <strong>Events</strong>. And all the features these events might have are called <strong><em>Event attributes</em></strong>. Say for example an app developer might want to track everytime a user adds something in the cart. So the event will be something like <strong>Product added to Cart</strong> with event attributes such as <em>name</em>, <em>price</em>, <em>discount</em>, <em>product_category</em> and etc.</p></li>
</ul>


<p>So now we have all the the available data that we track we can use to target users.</p>

<p>And I was tasked to create a service that basically returns a list of users based on a <strong>segment</strong> that is defined by a marketer or anyone else that has access to the moenagege dashboard. This particular service can be used by lot of other services to do other things, like push message workers will consume the users returned by this <strong>segmentation</strong> service to send push messages, or an email worker to send emails, or this same serivce can be used to create analytic dashboards to show how a given segment is varying with time, Or a smart trigger worker(smart triggers are basically some sort of interactive push message, as in there will be some trigger defined and if a user triggers it he will get a push message. Say a user adding some product to cart but not purchasing in the next 10 mins might trigger a push message, that guy probably deserved a push message XD). All these different services uses the segmentation service one way or the other. Some sample <em>segments</em> might look like as follows. A <em>segment</em> can just be all the users of the app, or all the users except the users from another segment.</p>

<p><img src="http://i.imgur.com/K6NyFX4.png" alt="Segment 1" />
<img src="http://i.imgur.com/KqoKfgZ.png" alt="Segment 2" />
<img src="http://i.imgur.com/NNf0upN.png" alt="Segmentation Dashboard" /></p>

<h1>Segmentation</h1>

<h2>Definitions</h2>

<p>Just remember this blindly, the output of any <strong>segmentation</strong> is always a set of unique userids/users.</p>

<p>As shown in the above picture. A <strong>segment</strong> consists one or more <strong>filters</strong>. And there are three kinds of filters.
* User segmentation filter(get all users whose <em>city</em> attribute is <em>London</em>)
* Event/datapoint segmentation filter(get all users who did <em>product purchased</em>)
* All Users(just all the users)</p>

<p>Sometimes a filter can be of type <strong>has not executed</strong> where you get a compute a filter and then subtract from <strong>all users</strong>
And the segment can be <strong>OR</strong> of all these filters or an <strong>AND</strong> of all these filters.</p>

<p>In one of the hackathons I built a nested <strong>AND</strong> or <strong>OR</strong> combinations of filters. To enable much more complex segments.</p>

<p>Its so surprising what sorts of use cases the customer success team used to bring to my attention, and ask me how to do this that. Every time they   have some edge case I had to fit that request with the existing dashboard and some basic set theory. Given all that, the dashboard shown above used to cover most cases. There are some cases where a single sigment contained more than 30 filters. The main challenge comes because of the disconnect where the guy who tracks the types of events(mobile app developer using sdk from app) and the person who ends up creating push campaigns (marketer who uses moengage dashboard) are from completely two different departments. And the sdk guy tracks all sorts of trivial nonsense and try to be comprehensive and the marketer has to somehow make use of the data that was being tracked and create meaningful push campaigns. I had to simultaneously handle both of their use cases and this created some unique challenges.</p>

<p><img src="http://i.imgur.com/DA6xihK.png" alt="Imgur" /></p>

<h2>Initail state when I joined</h2>

<p>When I first joined the most of the working segmentation part is just the <strong>All Users</strong> segment and a little bit of <strong>User segmentation</strong>. The first part is basically get a cursor on the User db and iterate till we get all the user ids and return it. In the initial days, the biggest user db is less than 10million. And it takes some 3-4 mins to return all the users. And second part is a little bit of user segmentation. Where its a straight forward query, but the main problem here is all user attributes need to be indexed, which is not reasonable to expect a mongo database to do. Not just that, the user attributes are not fixed, our clients are free to introduce what ever they want. Then again you can put all the attributes in a single dict object and index that one particular field. This is actually possible in mongo. But the performance is not really that great. And datapoints/event segmentation is basically going to be an aggregation query on the &lsquo;userid&rsquo; field with the given parameters. This is also implemented on mongo, but it wont work for any big aggregation queries on mongo. The schema of a user object and a data point object will give you some clarity of the ideas I just described and what I&rsquo;m going to do in the rest of the article.</p>

<p>Sample User Object:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{ 
</span><span class='line'>    id: userId,
</span><span class='line'>    name: John Wick,
</span><span class='line'>    city: London,
</span><span class='line'>    phone: 911,
</span><span class='line'>    last_seen: "2017-08-02 11:48:38.509285",
</span><span class='line'>    location: "40.748059, -33.312945",
</span><span class='line'>    age: 31,
</span><span class='line'>    sex: male,
</span><span class='line'>    sessions: 32,
</span><span class='line'>    status: active,
</span><span class='line'>    pushToken: "asdfasAs98787Dfasd",
</span><span class='line'>    os: ANDROID
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Sample Datapoint Object:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The output of a <strong>segmentation</strong> query is always a list of userIds, so from the above sample objects its clear that, if its a user filter, the query on database is straight forward query, and if its a datapoing filter, the query on the database is an aggregation query.</p>

<p>So the initial implementation of datapoint filters is also on mongo, where the event attributes are also not fixed. So the query on mongo is a normal filter and then a aggregation on userIds. Just the normal filter query will kill mongo servers if the datapoints are too many. And that will be the case normally.</p>

<p>All these filters will be queried seperately and the resultant of user ids are unioned or intersected using python set operations. This is basically limited by the memory of the segmentation worker.</p>

<p>Clearly all these are major scaling problems and the inital clients we had are very few in the early days of the company, and even then datapoint filters are not working.</p>

<h2>Initial Research</h2>

<p>I cant find all the references right now, but my intial research(which was some 2 years back) was pointing towards some sort of search engine database. I knew by then we need to look into Elasticsearch and Apache Solr. And I came across an opensource project called <a href="https://github.com/snowplow/snowplow">Snowplow</a> which is basically &ldquo;Enterprise-strength web, mobile and event analytics, powered by Hadoop, Kafka, Kinesis, Redshift and Elasticsearch&rdquo;. Sounded almost like what we wanted. Although we are not an analytics company. This played a major role in going ahead with Elasticsearch. I also had to decide between Apache Solr and Elasticsearch, but went ahead with ES given its slightly new, active and most of the comparision articles had lots of nice things to say about how its easier to manage the distributed aspects of ES than it is with Apache Solr.</p>

<h2><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a></h2>

<p>From the elasticsearch website, &ldquo;Elasticsearch is a distributed, JSON-based search and analytics engine designed for horizontal scalability, maximum reliability, and easy management.&rdquo;</p>

<p>Elasticsearch is built upon apache Lucene, just like Apache Solr, and added a nice layer that takes care of the distributed aspect of the horizontal scalability. Also provides a nice REST api to handle all sorts of operations like cluster management, node/cluster monitoring, CRUD operations etc.</p>

<p>In elasticsearch, by default every field(column) in the json object is indexed and can be searched, unlike usual databases like mongo, mysql and etc, you explicitly specify what fields to not be indexed. In traditional databases you specify what fields to be indexed. And it is horizontally scalable, you can add machines dynamically to the cluster as your data gets larger and when the master node discovers the newly added node, the master node will distribute all the data shards taking into consideration the newly added node.</p>

<p>It can be started on your local machine as a single node cluster or can be on hundreds of nodes.</p>

<h2>Basics of Elasticsearch.</h2>

<p>Lets start with a basic search object and go all the way up to the elasticsearch cluster and make ourself comfortable with all the ES specific terms that come up on our way.</p>

<p>An elasticseach cluster can be visualized in the image below.
<img src="http://i.imgur.com/VAAVpeH.png" alt="Imgur" /></p>

<p>Say for example in our case, take the event/datapoint json object that has to be searched for <strong>segmentation</strong> is</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The above datapoint belongs to a given <strong>type</strong> of an <strong>index</strong>. For all practical purposes you can ignore <strong>type</strong>, I never really made use of it, I did define that a datapoint object is of <strong>type</strong> datapoint and that&rsquo;s it. And a datapoint resides in one of the <strong>shards</strong>. It can be either a primary shard or a replication/secondary shard. All these primary and secondary <strong>shards</strong> combine to make up one <strong>index</strong>. All these shards are distributed over the <strong>cluster</strong>. A <strong>cluster</strong> can be a single machine or a bunch of <strong>nodes</strong>.</p>

<p>For example in the picture below, <strong>books</strong> is the name of the <strong>index</strong>. And while creating this <strong>index</strong> we defined it to have to have 3 shards and replicated twice. So there are a total of 6 shards, 3 primary(whiter green) shards and 3(dark green) secondary shards. Our datapoint object can be in any of those shards. All these shards are distributed over three elasticsearch <strong>nodes</strong>. And all these three nodes make up the <strong>cluster</strong>. The node with bolded star is the master node. It coordinates all the nodes to be in sync, When you do a search query, it decides based on the metadata it has which shards in what nodes to be queried.</p>

<p>I&rsquo;m writing this blogpost to explain how I made use of elasticsearch and fit it to our needs of segmenation. In further parts of the blog, I will be going in detail about es specific things as we go come across them.</p>

<p><img src="http://i.imgur.com/cYJqT4j.png" alt="Imgur" /></p>

<h2>Installation:</h2>

<p>Elasticsearch needs java to be installed. In the below gist you can see how to install elasticsearch. And start it as service so that it starts as soon as the machine starts. It also shows you how to install various elasticsearch plugins like <strong>kopf</strong>, <strong>bigdesk</strong>, <strong>ec2 discover</strong> plugin.</p>

<ul>
<li><strong>kopf</strong> that gives us a glimpse of the elasticsearch cluster, above screenshots</li>
<li><strong>bigdesk</strong> similar cluster state plugin</li>
<li><strong>ec2</strong> plugin that helps the discovery of the nodes in a cluster in a ec2, you can set discovery based on tags, security groups and other ec2 parameters. In the config file you can see the node discovery settings.</li>
</ul>


<p>After installation of elasticsearch as a linux service, you can find the config file at <code>/etc/elasticsearch/elasticsearch.yml</code> where you define the name of the node, cluster, ec2 discovery settings. And restart the service as shown. Ideally for a elasticsearch node that is in production you should reserve half of the available memory for jvm heap. You can find the settings for this <code>/etc/default/elasticsearch</code>. And you can find the log files in <code>/var/log/elasticsearch/</code>. You will find slowlogs, cluster/node startup logs and etc here.</p>

<script src="https://gist.github.com/syllogismos/6ca5c5ac3e89bfd314f0.js"></script>


<p>Okay this is a brief explanation of what ES is, and how to install. Lets dive into more details about how ES fits into the problem we were trying to solve in the first place. I haven&rsquo;t given more details of Elasticsearch, I will explain new concepts related to ES as we come across by implementing <strong>segmentation</strong>.</p>

<h2>Segmentation Challenges.</h2>

<p>The immediate major challenge we had was, the data point segmentation. In the initial days the biggest datapoint db has around 60 million objects. On mongo the datapoint queries/aggregations are just not working. And our first clients were just using basic user segmentation and <strong>All users</strong> segmentation.</p>

<p>Even with just 60 million objects the data point segmentation looked like a very hard problem then. After switching to elasticsearch and after a year I was able to support segmentation on 10 billion datapoint objects. We ended up having two elasticsearch clusters with ~20 nodes(32GB memory) in each just for datapoints. Things that weren&rsquo;t working started working, and at a scale of 166x after one year. The ride wasn&rsquo;t smooth, many sleepless nights, stress, hence many lessons learned along the way.</p>

<p>After <strong>datapoint segmentation</strong> was live for few days we moved the <strong>user segmentation</strong> also to elasticsearch. As having indexes on all the fields a user object on mongo is starting to stress our mongo db servers.</p>

<p>And as we got more clients, we were tracking almost 1/5th of Indian mobile users. And getting <strong>All users</strong> directly from cursor and iterating all the users everytime there is an all users request is not feasible anymore. As a db with 30 Million user objects is taking some 10 mins just to get the user ids present in the database. I also had to come up with reliable fast solution for this.</p>

<p>In further segments, we will discuss, each of the above three segmentation/filters use cases.</p>

<ul>
<li>Datapoint segmentation filter</li>
<li>User segmentation filter</li>
<li>All Users.</li>
</ul>


<h2>Datapoint/Event Segmentation Filter:</h2>

<p>Once again, the schema of the datapoint looks like this.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    product_category: "Electronics",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Above datapoint the event being tracked is <code>Added To Cart</code> of user <code>userId</code> and with rest of them as event attributes like <code>price</code>, <code>color</code>, <code>platform</code>, <code>product_name</code>.
A single app can track any number of such events, and each event will have any number of attributes, of all major datatypes, <code>location</code>, <code>int</code>, <code>string</code>. We call these event attributes.</p>

<p>Once again reiterating, the output of a segmentation query is a list of users.</p>

<p>Sample Event filters might look like this.</p>

<ul>
<li>Get all the users who did

<ol>
<li>the event <code>Added to Cart</code>,</li>
<li>exactly <code>3 times</code>,</li>
<li>with event attribute <code>Product Category</code> is <code>Electronics</code></li>
<li>in the last <code>7 days</code>.</li>
</ol>
</li>
<li>Get all the users who did

<ol>
<li>the event <code>Song Played</code>,</li>
<li>atleast <code>3 times</code>,</li>
<li>with event attribute <code>Song Genre</code> contains <code>Pop</code></li>
<li>with event attribute <code>Song Year</code> is equal to <code>2017</code></li>
<li>in the last <code>1 day</code></li>
</ol>
</li>
</ul>


<p>As you can see a event filter will always be an exact match on the <code>event</code>, the event attribute can be of type, <code>int</code>, or <code>string</code> or <code>datetime</code> or <code>location</code>. We also provide a condition of how many times a user did this particular event with the above event filters in the last <code>n days</code>.</p>

<p>Any event filter have these 4 parts.
1. What event you are interested in. eg <code>Added to Cart</code>
2. Event attribute conditions,
    * if event attribute is string, conditions like <code>is</code>, <code>contains</code>, <code>starts with</code>, and etc
    * event attribute is int, conditions like <code>equal to</code>, <code>greater than</code>, <code>less than</code>
3. Date condition <code>in the last n days</code> that specifies, how long in the past are you interested in. As a business decision we only support segmentation on the last 90 days of the data. Any data older than that will be deleted. Every day. As it doesn&rsquo;t make any sense to send push messages based on the user behaviour based on 90 days/3 months old data.
4. And we also support an extra condition based on how many times a given user did the above event with above event attribute conditions. Like say for the event <code>Logged In</code> you get get all the users who logged in <code>exactly 3 times</code> or <code>atleast 3 times</code> or <code>atmost 3 times</code></p>

<p>The earlier mongo implementation, all the events are stored in a single mongo <code>db</code> called <code>datapoints</code>. From the few amount of clients we had in the beginning, it is clear that the <code>events</code> people tracked are vastly different just based on the volume. Some events like <code>opened main page</code> will be in 10s of millions every day. And <code>product purchased</code> event would just be in thousads. And <code>product purchased</code> event might be of more interest when it came to segmentation queries. As users who did that event are much more important. So the volume of a given event and the significance of that event are both completely unrelated. An event of low volume might be of very high importance. And not just that, the developers might track every little interaction of the users. But the marketing folk might only be interested in 3-10 events.</p>

<p>Two major challenges.
1. Same app might track two different events that vary in volume, some events can be in 10&rsquo;s of millions while other events are just in 1000s
2. Even though an app might track 100s of events, there are only 3-10 events that the marketers are interested in.</p>

<p>So the first major diversion from the mongo implementation is having a seperate db(index in elasticsearch terms) for each event, as search performance on one event shouldn&rsquo;t be dependent on all the rest of the events.</p>

<p>So all the <code>Added to Cart</code> events of app <code>Amazon</code> would be in the elasticsearch index <code>amazon-addedtocart</code>. This is the first major decision that we struck with for a long time, even though the two major challenges are not completely solved. It made things a lot easier. In the initial version of Elasticsearch implementation, we decided on having 2 shards per index, no matter the volume of the index. There is a concept of <code>index aliasing</code> in elasticsearch that helps in dealing with very high volume events. In elasticsearch the name of the index cant be changed once it is created, so are the nubmer of shards. Index aliasing helps us with having more than one name/alias to query an index. A single elasticsearch index can have more than one alias. And more than one index can all have the same alias. So that when we query using an alias, all the indexes with that alias will be queried. But when you are inserting a object into an index, using an alias, that alias must point to only one index. So a <code>read alias</code> that you can use to query filters can point to more than one index. And a <code>write alias</code> must point to only one index for write operations to be successful. This concept of index aliasing helps us in dealing with really large indexes. As the number of shards per index is fixed after an index is created using alising you can direct new documents of a really big index to another document using <code>write alias</code> while searchs point to both the old index and the new index.</p>

<p>In elasticsearch by default the all the fields are indexed, there is an extra field called <code>_all</code> that considers the entire document as a single field. Indexing happens according to the type of <code>analyzer</code> you select. Our segmentation queries don&rsquo;t really require the default analyzer that is provided. The analyzer defines how the fields are tokenized and etc. So I had to disable analysis, so that the fileds are considered as they are provided.</p>

<p>All the settings of number of shards, index aliases, and analysis settings for string fields are set using <code>index templates</code>. An index templete can be used to set all types of settings based on the name of the index while it is being created. A sample index template might look like this.</p>

<script src="https://gist.github.com/syllogismos/824b069b4df415c3f2c3.js"></script>


<p>Using the above template we are doing
1. making default no of shards as 1 per index
2. creating an alias to all indexes, just by appending <code>-alias</code> to the end. So the index <code>amazon-addedtocart</code> can also be queried using <code>amazon-addedtocart-alias</code>. In the earlier implementation I didnt use aliases to the full potential, but I knew aliases will save by bum so I put an alias to every index that is created as above and it did help me to deal with very big indexes.
3. disalble indexing <code>_all</code> filed, which we don&rsquo;t really use in segmentation.
4. change the default analyzer to no analysis on all the string fields.</p>

<p>With the above template settings, I started a cluster with 4 nodes, each of 16GB memory, and out of the 4, 3 master eligible nodes. And started porting event data from mongo to elasticsearch using a open source tool called elaster. If I remember correctly, when we first moved to elasticsearch I was porting 60Million documents. On the other hand we were writing live tracking data from webworkers into elasticsearch.</p>

<h3>Query Language:</h3>

<h3>Stats and miscellanious notes:</h3>

<ul>
<li>Using this architecture, I scaled the segmentation service from initial 60 million objects to almost 10-20Billion datapoints.</li>
<li>From mongo to initial 4 nodes of 16Gb cluster to a total of ~40-45 nodes of 32 Gb memory.</li>
<li>Total number of shards around ~20k</li>
<li>There is a concept of snapshots in elasticsearch, Along with this we also had a script that takes nightly backups of data and dumps it in S3. We also collect backups of the raw http api requests in s3, that goes in from kafka. But more backups never hurt anybody.</li>
<li><p>And we only supported 90 days of tracking data, so we had to delete data older than 90 days everyday. With data deletion also, there is a concept of <code>segments</code> on elasticsearch. Where objects reside on these segments. And when you delete an object it is just marked as delete and only when all the documents on a segment are deleted will they go away permenantly. Luckily for us we delete data date wise, and the sharding also happens based on date of creation. Although all the documents deleted might not have gone as soon as they were deleted, they eventually permenantly deleted.</p>

<h3>Issues:</h3></li>
<li><p>Type issues: One major problem we faced is, in elasticsearch once the type of a field is set to <code>int</code>, and if you are trying to insert any new document with that field containing a <code>string</code> it wont be inserted. This is a really huge gotcha, and if I was not careful I would be losing lots of event data. We always have to be so nice to the <code>users</code> no matter how dumb they seem to be, we just have to think of all the ways the services we are providing can be abused or misused. Say for example <code>cost</code> event attribute initially they are sending intigers. And later on decided they send the same thing by prepending a string <code>$</code>. All the new datapoints will be rejected. And there are some event attributes that exist in all datapoints. If by mistake they get mistyped when the index created, every datapoint that comes next will get rejected. To deal with this, using index templates, I set the type of the known fields beforehand. To deal with this,I put the datapoints that failed to be inserted into es because of type issues, in a mongo, and the entire json object as one string in <code>error-amazon</code>(in this example) index in the same cluster. Usually these type errors luckily for us are caused in test apps, while people are testing the integrating. So I used to deal with these errors case by case basis. I had scripts ready that clean up the data and fix type errors and reindex in the proper event index. But in later versions of the segmentation we came up with a really nice solution that permenantly solves this.</p></li>
<li><p>So from the above architecture, it is clear that there will be a seperate index, for every new event the client starts to track. We are providing a single endpoint through the sdk, and the user might accidentally pass a variable in the event name field, like say the <code>user id</code>. With us not providing any limit on the number of the events they can track. This will end up creating tens of thousands of indexes, which will just bottle neck the entire cluster And it will become a disaster. And there are cases where some of our clients passed <code>user id</code> as the name of the event. I had to go modify the workers code and do a release at some ungodly hour to fix this.</p></li>
<li><p>When an index gets too big, bigger than the heap size of the node that particular shard is reciding on. I had make use of the index aliasing opetion to write to a newer index while searching on both the old and new indexes. This is also one of the major problems, I faced, and directly affected the stability of the cluster. In later versions of the segmentation, using the alises, we automated whatever I was doing manually. And I will talk about these changes in later section.</p></li>
</ul>


<h2>User Segmentation filter:</h2>

<p>Eventually as we were getting more clients, doing the user segmentation filter queries on mongo db was also proving to be challenging. As we are querying on user db, the queries that hit the db are just normal filter queries. Apply all the filters and get a cursor, iterate through the cursor and return the list of users of a given segment. As with data point segmentation, we support all types to filter on the user db, <code>int</code>, <code>string</code>, <code>date</code>, <code>location</code>.</p>

<p>For mongo queries to work for user segmentation on all the user attributes that the app is tracking, we have a nested object that holds all the user attributes, and a single index on the the nested field. And as the user attributes are not fixed, and our clients can introduce anything they want to track. Having index such that queries work efficiently on all these fields is becoming challenging as the user dbs are getting bigger.</p>

<p>So eventually we wanted to move user segmentation also to elasticsearch. But the tricky part here is, that user objects recieve updates on it, unlike datapoints. Such as, last seen of the user keeps updating everytime the user uses it. Location of the user updates. This is fundamentally different from datapoints db. And the challenge now is if we should completely move away from mongo and switch to elasticsearch. But mongo infrastructure was heavily used by the push team to send push notifications once they are given the user ids. And we had to have mongo as the primary data source. And have elasticsearch that mirrors the data on mongo. We can&rsquo;t just write once and keep quite about it, we have to constantly update the elasticsearch data with new updates on user objects.</p>

<h3>River Hell:</h3>

<p>Above challenge brings us to a concept called <code>Rivers</code>, which exists to solve exactly this problem. The basic concept of rivers is that you have a primary db, some where like a mongodb/couchdb and etc. And you keep tailing all the CRUD operations that happen on the db and replay all those operations on elasticsearch. The river backend was provided by the elasticsearch backend in those days. And there exist several third party drivers that latch on to the CRUD operations that happen on the primary sources, like mongo/couchdb and etc.</p>

<p>Conceptually <code>Rivers</code> are what we needed. And I started the user elasticsearch cluster with some 6 nodes that serve user segmentation queries and started the rivers on all the bigger production clients. Things seemed to work well. Except RIVERS ARE UTTER CRAP AND VERY UNRELIABLE, and no wonder they were deprecated by elasticsearch guys as well.</p>

<p>They used to run fine, but after few days, the nodes used to max out on the jvm or memory and the entire user segmentation elasticsearch cluster used to not respond. I have to restart the questionable nodes, and after restarting the nodes, because of the replication shards, the user db indexes were back, but the individual river stopped and CRUD updates on the primary db stopped reflecting on the elasticsearch. So basically the user data on elasticsearch was stale. So I have to delete the index, restart the river from the beginning, and while this restarting is happening, I had to redirect the segmentatino queries on to mongo db for backup. It was a mess. Rivers turned out to be such a pain to work with, it almost became a running joke in the entire office. I built a crude river dashboard that shows the status of the rivers and the difference in the number of users in elasticsearch index and the mongo db, this difference usually was a good indicator of the rivers status. And alerts that alert me when a river stops updating new users. Rivers are the most unreliable thing I had to use, and there is a good reason why it got deprecated.</p>

<h3>River Alternative:</h3>

<p>I was already looking at alternatives for rivers, and I know that things like <a href="https://github.com/mongodb-labs/mongo-connector">mongo connector</a> exist. But the question is how reliable that thing will be. But we got to know it was working well for similar use case from a asking around. Mongo already has a way of latching onto and tailing all the crud operations(oplog) and replaying all that. That is how the secondary dbs also replicate their data from the primary mongo nodes. And it is fairly robust. And the mongo connector provides us a way to make use of this replication and index into elasticsearch. Basically we are indexing stuff to elasticsearch(which it is supposed to be good at) and that stuff is basically obtained from oplog of the mongodb which is also fairly robustly implemented. So using mongo connector we are making use of things in which both mongo and elasticsearch are good at in their respective tasks.</p>

<p>By now I had a proper team mate and I made him fork mongo connector and replicate all the river functionality that we were using earlier and made the switch from rivers to mongo-connector.</p>

<h3>Stats:</h3>

<p>I think by then we were supporting user segmentation on around 80-100Million users.</p>

<h2>All Users:</h2>

<p>In the end after moving to Elasticsearch</p>

<h2>Challenges, elasticsearch quirks</h2>

<h2>datapoint segmentation, some aggregation queries</h2>

<h2>user segmenatation to elasticsearch, rivers bullshit and mongo connector.</h2>

<h2>backup,</h2>

<h2>challenges while testing, smaller apps are different than bigger apps</h2>

<h2>all users cache implementation</h2>

<h2>custom segments</h2>

<h2>overall diagram</h2>

<h2>Challenges with current state, types of problems faced app-action indexes, case insensitive, suggestions, os specific..</h2>

<h2>Rearchitecture</h2>

<h2>event management dashboard, disconnect between app developers and marketers.</h2>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">syllogismos</span></span>

      




<time class='entry-date' datetime='2017-07-30T12:11:45+05:30'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>12:11 pm</span></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://syllogismos.github.io/blog/2017/07/30/elasticsearch-segmentation-moengage/" data-via="2abstract4me" data-counturl="http://syllogismos.github.io/blog/2017/07/30/elasticsearch-segmentation-moengage/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/01/02/santander-product-recommendation-kaggle/" title="Previous Post: Santander Product Recommendation Kaggle">&laquo; Santander Product Recommendation Kaggle</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/08/02/miscellanious-notes/" title="Next Post: Miscellanious Notes">Miscellanious Notes &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/08/02/miscellanious-notes/">Miscellanious Notes</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/07/30/elasticsearch-segmentation-moengage/">Elasticsearch, Segmentation, MoEngage</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/02/santander-product-recommendation-kaggle/">Santander Product Recommendation Kaggle</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/15/ghost-blog-as-your-github-profile-page/">Ghost Blog as Your Github User Page</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/13/stochastic-gradient-descent/">Stochastic Gradient Descent in AD.</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/syllogismos">@syllogismos</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'syllogismos',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - syllogismos -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
