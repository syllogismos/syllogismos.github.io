
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>My Blog</title>
  <meta name="author" content="syllogismos">

  
  <meta name="description" content="Just a collection of some notes and tips that helps me in my development workflow. SSH Quit an ssh session using CR~. and get to your local terminal &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://syllogismos.github.io/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="My Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">My Blog</a></h1>
  
    <h2>My learnings and etc.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="syllogismos.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/08/02/miscellanious-notes/">Miscellanious Notes</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-08-02T13:55:59+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>2</span><span class='date-suffix'>nd</span>, <span class='date-year'>2017</span></span> <span class='time'>1:55 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Just a collection of some notes and tips that helps me in my development workflow.</p>

<h2>SSH</h2>

<ul>
<li>Quit an ssh session using <code>CR~.</code> and get to your local terminal. <code>~</code> is the escape character for ssh and dot to quit ssh. But note that tilda has to follow new line. When ever your ssh connection gets disconnected for one reason or the other, say you lost internet connection or some other reason, sometimes the screen wont repsond to any keystroke and it seems like its struck in the ssh session. Its very annoying. Instead of closing the terminal, you can do this instead <code>CR~.</code></li>
<li>Add <code>ServerAliveInterval 60</code> to your ssh config file, most probably in <code>~/.ssh/config</code>. Usually by default your ssh connection closes if it is idle for a long time, by doing this you are pinging every 60 seconds to keep the connection alive. Very helpful as you don&rsquo;t have to connect to your remote machine again and again.</li>
</ul>


<h2>Useful command line tools</h2>

<ul>
<li>s3cmd, interact with your s3 buckets

<ol>
<li><code>pip install s3cmd</code></li>
<li><code>s3cmd --configure</code> # configure your keys</li>
<li><code>s3cmd ls</code> # list buckets, contents of a dir</li>
<li><code>s3cmd mb s3://new_bucket/</code> # make new bucket</li>
<li><code>s3cmd put --recursive localfiles s3://uri/</code> # copy local files to s3</li>
<li><code>s3cmd get --recursive s3://uri/</code> # get s3 files to local machine</li>
</ol>
</li>
<li>jq, parse json files and pretty print on terminal

<ol>
<li><code>brew install jq</code></li>
</ol>
</li>
<li>grep, unix search tool

<ol>
<li><code>grep -A2 query file.txt</code> # prints 2 lines <code>After</code> the queried term</li>
<li><code>grep -B2 query file.txt</code> # prints 2 lines <code>Before</code> the queried term</li>
<li><code>grep -C2 query file.txt</code> # prints 2 lines of <code>Context</code> on both sides</li>
<li><code>grep -e regex file.txt</code> # searches for the regexp</li>
</ol>
</li>
<li>sort, sorting text

<ol>
<li><code>sort -nr</code> # sorts numbers in descending order</li>
</ol>
</li>
<li>awk, command line programming language?

<ol>
<li><code>cat file.txt | awk '{print $2}'</code> # prints the second column</li>
<li>it does a lot more things, but this is what i end up using the most for</li>
</ol>
</li>
<li>head tail, head and tail of the files

<ol>
<li><code>tail -1000f file.txt</code> # last 1000 lines and waits for more lines that are being added to the file</li>
<li><code>head -n 25 file.txt</code> # first 25 lines</li>
</ol>
</li>
<li>gist, command line gist utility

<ol>
<li><code>brew install gist</code></li>
<li><code>gist --login</code> # loging to your github</li>
<li><code>gist -p file.txt</code> # upload private gist</li>
</ol>
</li>
<li>screen, window manager that multiplexs multiple terminal sessions

<ol>
<li><code>screen -rd</code> reattach to an existing secreen session even if its open somewhere else</li>
<li><code>Ctrl-a d</code> detach from an existing screen session</li>
<li><code>Ctrl-a ?</code> help menu in screen</li>
<li><code>Ctrl-a n</code> next terminal</li>
<li><code>Ctrl-a p</code> previous terminal</li>
<li><code>Ctrl-a k</code> kill the curernt temrinal in screen</li>
</ol>
</li>
</ul>


<h2>python</h2>

<ul>
<li><code>python -u python_script.py &gt; python_logs.txt</code> Usually I want to capture all the print statement logs in some text file, so that I can save them for further reference, instead of throwing them off in stdout. So above all my print logs will be in <code>python_logs.txt</code>, and I follow along the logs using <code>tail -100f python_logs.txt</code>. And the <code>-u</code> flag forces the print statements to be not bufferred while writing them to <code>python_logs.txt</code>. Other wise even if your program is running you wont find the logs in the log file as soon as they get executed.</li>
</ul>


<h2><a href="https://github.com/matryer/bitbar">Bitbar</a></h2>

<p>A utility that helps you write osx menu bar applications, and also use lots of community built plugins/menu bar applications.</p>

<p><code>brew install bitbar</code></p>

<h3>Community Plugins:</h3>

<ul>
<li><p><a href="https://github.com/matryer/bitbar-plugins/blob/master/Time/worldclock.1s.sh">Timezone plugin</a>, shows time in 4-5 major timezones.
<img src="http://i.imgur.com/OoCfxTn.png" alt="Timezone plugin" /></p></li>
<li><p><a href="">Clipboard plugin</a>, clipboard of the last 10 items copied, click on an item to copy it back to clipboard.</p></li>
</ul>


<h3>My Plugins:</h3>

<h4>EC2 Plugin</h4>

<p>My own ec2 bitbar plugin, that makes all my common EC2 tasks one click, and so much easier to do.
<img src="http://i.imgur.com/xG7rtCt.png" alt="Imgur" />
<img src="http://i.imgur.com/S3YOsfG.png" alt="Imgur" /></p>

<ol>
<li>I can start, stop, restart, teminate my running machines from the menu bar</li>
<li>Create a low cpu alarm, that sends me an alert email when the cpu is lower than 20%. This is what I use mostly cause I want to get an alert when my machine learning scripts are die/finish.</li>
<li>Delete the above created alarm.</li>
<li>Copy ssh command to the clipboard with the right private key file just by a single click. It so annoying everytime I start a machine in ec2, I have to copy the public ip/dns and type where the private key is and type <code>ubuntu@</code>. All of this is just single click away now.</li>
<li>Displays various machine types, their cores, memory, normal prices, current spot prices in all the regions I&rsquo;m interested in.</li>
<li>Lists all the available AMIs, and launch a spot instance from that AMI</li>
</ol>


<h4>KEYS plugin</h4>

<p><img src="http://i.imgur.com/zuuGgq4.png" alt="Imgur" /></p>

<p>By clicking on the above words, the respective thing will be copied to clipboard.</p>

<p>Server setup is to setup my vim/github settings to any new server that I start. which basically copies <code>wget https://github.com/syllogismos/dotfiles/raw/master/server_setup.sh</code> into clipboard. and <code>sh server_setup.sh</code> will update my server vimrc and gitconfig files.</p>

<h2>git</h2>

<ul>
<li>add this alias in your <code>~/.gitconfig</code> under <code>[alias]</code>, so that you can do <code>git hist</code> that shows pretty tree version of git commit history.
  <code>
  hist = log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short
 </code>
<img src="http://i.imgur.com/iktQDA1.png" alt="" />
<img src="http://i.imgur.com/JBTqcyN.png" alt="" /></li>
</ul>


<h2>vim</h2>

<p>You can find my vimrc <a href="https://github.com/syllogismos/dotfiles/blob/master/vim/vimrc.symlink">here</a></p>

<p>My favourite things in default vim are and find myself using them again and again.</p>

<ul>
<li>visual block, this is the feature I miss the most when I use any other editor like say vscode or any other. <code>Ctrl+Shift+v</code></li>
<li>recording, macro recording. <code>qa</code> to start recording your operations in the <code>a</code> register, and end the recording by typing <code>q</code> again. And repeat those operations by doing <code>@a</code>. I did <code>qa</code> for clarity of whats happening, but for speediness, I do <code>qq</code> to record in <code>q</code> register. And then end recording with <code>q</code>. And repeat with <code>@q</code>. You can also use <code>@@</code> to just repeat the previous macro operation.</li>
<li><code>.</code> a simple dot. it just repeats previous insert operation. Its so valuable and it surprises several times. Especially while programming, where things often repeat.</li>
</ul>


<p>Most used <code>vimrc</code> settings.</p>

<ul>
<li><code>set number</code> line number</li>
<li><code>set relativenumber</code> shows lines numbers relative to the current line</li>
<li><code>set scrolloff=3</code> while scrolling all the way down or all the way up it gives you 3 lines of context instead of the default. It starts scrolling when you reach the last 3 lines instead of the bottom.</li>
<li><code>inoremap jk &lt;esc&gt;</code> and <code>inoremap kj &lt;esc&gt;</code> escape key is soo far away.. and I just type <code>jk</code> or <code>kj</code> quickly to get to normal mode from insert mode.</li>
<li><code>inoremap ,,  &lt;C-p&gt;</code> intellisense of sorts by doing <code>,,</code> quick completions but only works for the words that already came earlier.</li>
<li><code>noremap &lt;Space&gt; :w&lt;Esc&gt;</code> space bar to save the file in normal mode</li>
<li><code>noremap  &lt;buffer&gt; &lt;silent&gt; k gk</code> and <code>noremap  &lt;buffer&gt; &lt;silent&gt; j gj</code> when lines are wrapped around the width and a single line takes up more than one line, normally you have to type <code>gk</code> to go down but its annoying so binding <code>k</code> to <code>gk</code> makes things easier.</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/07/30/elasticsearch-segmentation-moengage/">Elasticsearch, Segmentation, MoEngage</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-07-30T12:11:45+05:30'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>30</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>12:11 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>For almost two years I worked at a company called <a href="https://moengage.com/">MoEngage</a>, which is a marketing automation b2b company for app developers. We handle push messages, inapp messages for both mobile phone apps and web apps, email campaigns etc,. I was one of the early employees and I worked on <strong>Segmentation</strong> and built it from ground up. The early days were probably my favourite days, everyone worked with so much intensity at a very fast pace with no scope for any distractions. Very productive and rewarding work. My work touched almost all aspects of the business and the entire tech stack we were using. I mainly want this to be a tech blog post about the implementation of <strong>segmentation</strong> using <strong>elasticsearch</strong> but also share some learning experiences being part of a budding young startup with no prior experience of building an engineering team., and I regret so much not writing this earlier, now that it&rsquo;s been a while me quitting that job, I definitely wont be able to be as comprehensive as I would like it to be. I was very comfortable with the elasticsearch, celery workers, redis, s3, sqs, aws, python debugging, release process, testing at scale etc., I faced and solved very interesting and unique challenges specific to our use case. Please forgive me for any grammatical or spelling mistakes. I postponed this post for the single reason of wanting to do it perfectly, but the exact opposite is happening. Now I just can&rsquo;t wait anymore to get this out.</p>

<h1>Problem statement and context.</h1>

<p>As I said, briefly Moengage is a marketing automation solution for apps, using moengage, apps can send push messages to users of an app and target a very specific  segment of users and reduce churn. At that point of time the mvp of moengage is that we can target users very specifically based on their behaviour inside the app or their own characteristics like their age/sex/location and other attributes. But none of that is built yet. And the few clients that were using us were just bulk messaging(<em>spamming</em>) all the users with the same push message. Just get a cursor on the user database, iterate through them all and send each user to the push worker and finally send the push messages. Turns out even this is hard for apps to do, they would rather use some 3rd party to do handle the push infrastructure and they just have a nice dashboard where their marking folk can put in what push message to send, like offers, deals and other stuff.</p>

<p>We provide sdk to the app developers where we give them two main end points. The sdk does a lot more than this, but for this blog post the only endpoints that we are concernen with are the below two</p>

<ul>
<li><p>One is to track <strong>Users</strong>. This endpoint can send stuff like name, location, city, sex, age, email. Everything that can be considered as a feature of the user. We call these features <strong><em>User attributes</em></strong></p></li>
<li><p>And another is to track what the user is doing inside the app we call these <strong>Events</strong>. And all the features these events might have are called <strong><em>Event attributes</em></strong>. Say for example an app developer might want to track every time a user adds something in the cart. So the event will be something like <strong>Product added to Cart</strong> with event attributes such as <em>name</em>, <em>price</em>, <em>discount</em>, <em>product_category</em> and etc.</p></li>
</ul>


<p>So now we have all the the available data that we track we can use to target users.</p>

<p>And I was tasked to create a service that basically returns a list of users based on a <strong>segment</strong> that is defined by a marketer or anyone else that has access to the moenagege dashboard. This particular service can be used by lot of other services to do other things, like push message workers will consume the users returned by this <strong>segmentation</strong> service to send push messages, or an email worker to send emails, or this same service can be used to create analytic dashboards to show how a given segment is varying with time, Or a smart trigger worker(smart triggers are basically some sort of interactive push message, as in there will be some trigger defined and if a user triggers it he will get a push message. Say a user adding some product to cart but not purchasing in the next 10 mins might trigger a push message, that guy probably deserved a push message XD). All these different services uses the segmentation service one way or the other. Some sample <em>segments</em> might look like as follows. A <em>segment</em> can just be all the users of the app, or all the users except the users from another segment.</p>

<p><img src="http://i.imgur.com/K6NyFX4.png" alt="Segment 1" />
<img src="http://i.imgur.com/KqoKfgZ.png" alt="Segment 2" />
<img src="http://i.imgur.com/NNf0upN.png" alt="Segmentation Dashboard" /></p>

<h1>Segmentation</h1>

<h2>Definitions</h2>

<p>Just remember this blindly, the output of any <strong>segmentation</strong> is always a set of unique userids/users.</p>

<p>As shown in the above picture. A <strong>segment</strong> consists one or more <strong>filters</strong>. And there are three kinds of filters.
* User segmentation filter(get all users whose <em>city</em> attribute is <em>London</em>)
* Event/datapoint segmentation filter(get all users who did <em>product purchased</em>)
* All Users(just all the users)</p>

<p>Sometimes a filter can be of type <strong>has not executed</strong> where you get a compute a filter and then subtract from <strong>all users</strong>
And the segment can be <strong>OR</strong> of all these filters or an <strong>AND</strong> of all these filters.</p>

<p>In one of the hackathons I built a nested <strong>AND</strong> or <strong>OR</strong> combinations of filters. To enable much more complex segments.</p>

<p>Its so surprising what sorts of use cases the customer success team used to bring to my attention, and ask me how to do this that. Every time they   have some edge case I had to fit that request with the existing dashboard and some basic set theory. Given all that, the dashboard shown above used to cover most cases. There are some cases where a single segment contained more than 30 filters. The main challenge comes because of the disconnect where the guy who tracks the types of events(mobile app developer using sdk from app) and the person who ends up creating push campaigns (marketer who uses moengage dashboard) are from completely two different departments. And the sdk guy tracks all sorts of trivial nonsense and try to be comprehensive and the marketer has to somehow make use of the data that was being tracked and create meaningful push campaigns. I had to simultaneously handle both of their use cases and this created some unique challenges.</p>

<h2>Initial state when I joined</h2>

<p>When I first joined the most of the working segmentation part is just the <strong>All Users</strong> segment and a little bit of <strong>User segmentation</strong>. The first part is basically get a cursor on the User db and iterate till we get all the user ids and return it. In the initial days, the biggest user db is less than 10million. And it takes some 3-4 mins to return all the users. And second part is a little bit of user segmentation. Where its a straight forward query, but the main problem here is all user attributes need to be indexed, which is not reasonable to expect a mongo database to do. Not just that, the user attributes are not fixed, our clients are free to introduce what ever they want. Then again you can put all the attributes in a single dict object and index that one particular field. This is actually possible in mongo. But the performance is not really that great. And datapoints/event segmentation is basically going to be an aggregation query on the &lsquo;userid&rsquo; field with the given parameters. This is also implemented on mongo, but it wont work for any big aggregation queries on mongo. The schema of a user object and a data point object will give you some clarity of the ideas I just described and what I&rsquo;m going to do in the rest of the article.</p>

<p>Sample User Object:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{ 
</span><span class='line'>    id: userId,
</span><span class='line'>    name: John Wick,
</span><span class='line'>    city: London,
</span><span class='line'>    phone: 911,
</span><span class='line'>    last_seen: "2017-08-02 11:48:38.509285",
</span><span class='line'>    location: "40.748059, -33.312945",
</span><span class='line'>    age: 31,
</span><span class='line'>    sex: male,
</span><span class='line'>    sessions: 32,
</span><span class='line'>    status: active,
</span><span class='line'>    pushToken: "asdfasAs98787Dfasd",
</span><span class='line'>    os: ANDROID
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Sample Datapoint Object:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The output of a <strong>segmentation</strong> query is always a list of userIds, so from the above sample objects its clear that, if its a user filter, the query on database is straight forward query, and if its a datapoint filter, the query on the database is an aggregation query.</p>

<p>So the initial implementation of datapoint filters is also on mongo, where the event attributes are also not fixed. So the query on mongo is a normal filter and then a aggregation on userIds. Just the normal filter query will kill mongo servers if the datapoints are too many. And that will be the case normally.</p>

<p>All these filters will be queried separately and the resultant of user ids are unioned or intersected using python set operations. This is basically limited by the memory of the segmentation worker.</p>

<p>Clearly all these are major scaling problems and the initial clients we had are very few in the early days of the company, and even then datapoint filters are not working.</p>

<h2>Initial Research</h2>

<p>I cant find all the references right now, but my initial research(which was some 2 years back) was pointing towards some sort of search engine database. I knew by then we need to look into Elasticsearch and Apache Solr. And I came across an open source project called <a href="https://github.com/snowplow/snowplow">Snowplow</a> which is basically &ldquo;Enterprise-strength web, mobile and event analytics, powered by Hadoop, Kafka, Kinesis, Redshift and Elasticsearch&rdquo;. Sounded almost like what we wanted. Although we are not an analytics company. This played a major role in going ahead with Elasticsearch. I also had to decide between Apache Solr and Elasticsearch, but went ahead with ES given its slightly new, active and most of the comparison articles had lots of nice things to say about how its easier to manage the distributed aspects of ES than it is with Apache Solr.</p>

<h2><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a></h2>

<p>From the elasticsearch website, &ldquo;Elasticsearch is a distributed, JSON-based search and analytics engine designed for horizontal scalability, maximum reliability, and easy management.&rdquo;</p>

<p>Elasticsearch is built upon apache Lucene, just like Apache Solr, and added a nice layer that takes care of the distributed aspect of the horizontal scalability. Also provides a nice REST api to handle all sorts of operations like cluster management, node/cluster monitoring, CRUD operations etc.</p>

<p>In elasticsearch, by default every field(column) in the json object is indexed and can be searched, unlike usual databases like mongo, mysql and etc, you explicitly specify what fields to not be indexed. In traditional databases you specify what fields to be indexed. And it is horizontally scalable, you can add machines dynamically to the cluster as your data gets larger and when the master node discovers the newly added node, the master node will distribute all the data shards taking into consideration the newly added node.</p>

<p>It can be started on your local machine as a single node cluster or can be on hundreds of nodes.</p>

<h2>Basics of Elasticsearch.</h2>

<p>Lets start with a basic search object and go all the way up to the elasticsearch cluster and make ourself comfortable with all the ES specific terms that come up on our way.</p>

<p>An elasticseach cluster can be visualised in the image below.
<img src="http://i.imgur.com/VAAVpeH.png" alt="Imgur" /></p>

<p>Say for example in our case, take the event/datapoint json object that has to be searched for <strong>segmentation</strong> is</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>The above datapoint belongs to a given <strong>type</strong> of an <strong>index</strong>. For all practical purposes you can ignore <strong>type</strong>, I never really made use of it, I did define that a datapoint object is of <strong>type</strong> datapoint and that&rsquo;s it. And a datapoint resides in one of the <strong>shards</strong>. It can be either a primary shard or a replication/secondary shard. All these primary and secondary <strong>shards</strong> combine to make up one <strong>index</strong>. All these shards are distributed over the <strong>cluster</strong>. A <strong>cluster</strong> can be a single machine or a bunch of <strong>nodes</strong>.</p>

<p>For example in the picture below, <strong>books</strong> is the name of the <strong>index</strong>. And while creating this <strong>index</strong> we defined it to have to have 3 shards and replicated twice. So there are a total of 6 shards, 3 primary(whiter green) shards and 3(dark green) secondary shards. Our datapoint object can be in any of those shards. All these shards are distributed over three elasticsearch <strong>nodes</strong>. And all these three nodes make up the <strong>cluster</strong>. The node with bolded star is the master node. It coordinates all the nodes to be in sync, When you do a search query, it decides based on the metadata it has which shards in what nodes to be queried.</p>

<p>I&rsquo;m writing this blogpost to explain how I made use of elasticsearch and fit it to our needs of segmentation. In further parts of the blog, I will be going in detail about es specific things as we go come across them.</p>

<p><img src="http://i.imgur.com/cYJqT4j.png" alt="Imgur" /></p>

<h2>Installation:</h2>

<p>Elasticsearch needs java to be installed. In the below gist you can see how to install elasticsearch. And start it as service so that it starts as soon as the machine starts. It also shows you how to install various elasticsearch plugins like <strong>kopf</strong>, <strong>bigdesk</strong>, <strong>ec2 discover</strong> plugin.</p>

<ul>
<li><strong>kopf</strong> that gives us a glimpse of the elasticsearch cluster, above screenshots</li>
<li><strong>bigdesk</strong> similar cluster state plugin</li>
<li><strong>ec2</strong> plugin that helps the discovery of the nodes in a cluster in a ec2, you can set discovery based on tags, security groups and other ec2 parameters. In the config file you can see the node discovery settings.</li>
</ul>


<p>After installation of elasticsearch as a linux service, you can find the config file at <code>/etc/elasticsearch/elasticsearch.yml</code> where you define the name of the node, cluster, ec2 discovery settings. And restart the service as shown. Ideally for a elasticsearch node that is in production you should reserve half of the available memory for jvm heap. You can find the settings for this <code>/etc/default/elasticsearch</code>. And you can find the log files in <code>/var/log/elasticsearch/</code>. You will find slowlogs, cluster/node startup logs and etc here.</p>

<script src="https://gist.github.com/syllogismos/6ca5c5ac3e89bfd314f0.js"></script>


<p>Okay this is a brief explanation of what ES is, and how to install. Lets dive into more details about how ES fits into the problem we were trying to solve in the first place. I haven&rsquo;t given more details of Elasticsearch, I will explain new concepts related to ES as we come across by implementing <strong>segmentation</strong>.</p>

<h2>Segmentation Challenges.</h2>

<p>The immediate major challenge we had was, the data point segmentation. In the initial days the biggest datapoint db has around 60 million objects. On mongo the datapoint queries/aggregations are just not working. And our first clients were just using basic user segmentation and <strong>All users</strong> segmentation.</p>

<p>Even with just 60 million objects the data point segmentation looked like a very hard problem then. After switching to elasticsearch and after a year I was able to support segmentation on 10 billion datapoint objects. We ended up having two elasticsearch clusters with ~20 nodes(32GB memory) in each just for datapoints. Things that weren&rsquo;t working started working, and at a scale of 166x after one year. The ride wasn&rsquo;t smooth, many sleepless nights, stress, hence many lessons learned along the way.</p>

<p>After <strong>datapoint segmentation</strong> was live for few days we moved the <strong>user segmentation</strong> also to elasticsearch. As having indexes on all the fields a user object on mongo is starting to stress our mongo db servers.</p>

<p>And as we got more clients, we were tracking almost 1/5th of Indian mobile users. And getting <strong>All users</strong> directly from cursor and iterating all the users every time there is an all users request is not feasible anymore. As a db with 30 Million user objects is taking some 10 mins just to get the user ids present in the database. I also had to come up with reliable fast solution for this.</p>

<p>In further segments, we will discuss, each of the above three segmentation/filters use cases.</p>

<ul>
<li>Datapoint segmentation filter</li>
<li>User segmentation filter</li>
<li>All Users.</li>
</ul>


<h2>Datapoint/Event Segmentation Filter:</h2>

<p>Once again, the schema of the datapoint looks like this.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{
</span><span class='line'>    id: ObjectId,
</span><span class='line'>    userId: userId,
</span><span class='line'>    event: "Added To Cart",
</span><span class='line'>    product_name: "iPhone 64GB",
</span><span class='line'>    product_category: "Electronics",
</span><span class='line'>    price: 500,
</span><span class='line'>    color: "Red",
</span><span class='line'>    platform: "ANDROID"
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p>Above datapoint the event being tracked is <code>Added To Cart</code> of user <code>userId</code> and with rest of them as event attributes like <code>price</code>, <code>colour</code>, <code>platform</code>, <code>product_name</code>.
A single app can track any number of such events, and each event will have any number of attributes, of all major datatypes, <code>location</code>, <code>int</code>, <code>string</code>. We call these event attributes.</p>

<p>Once again reiterating, the output of a segmentation query is a list of users.</p>

<p>Sample Event filters might look like this.</p>

<ul>
<li>Get all the users who did

<ol>
<li>the event <code>Added to Cart</code>,</li>
<li>exactly <code>3 times</code>,</li>
<li>with event attribute <code>Product Category</code> is <code>Electronics</code></li>
<li>in the last <code>7 days</code>.</li>
</ol>
</li>
<li>Get all the users who did

<ol>
<li>the event <code>Song Played</code>,</li>
<li>at least <code>3 times</code>,</li>
<li>with event attribute <code>Song Genre</code> contains <code>Pop</code></li>
<li>with event attribute <code>Song Year</code> is equal to <code>2017</code></li>
<li>in the last <code>1 day</code></li>
</ol>
</li>
</ul>


<p>As you can see a event filter will always be an exact match on the <code>event</code>, the event attribute can be of type, <code>int</code>, or <code>string</code> or <code>datetime</code> or <code>location</code>. We also provide a condition of how many times a user did this particular event with the above event filters in the last <code>n days</code>.</p>

<p>Any event filter have these 4 parts.
1. What event you are interested in. eg <code>Added to Cart</code>
2. Event attribute conditions,
    * if event attribute is string, conditions like <code>is</code>, <code>contains</code>, <code>starts with</code>, and etc
    * event attribute is int, conditions like <code>equal to</code>, <code>greater than</code>, <code>less than</code>
3. Date condition <code>in the last n days</code> that specifies, how long in the past are you interested in. As a business decision we only support segmentation on the last 90 days of the data. Any data older than that will be deleted. Every day. As it doesn&rsquo;t make any sense to send push messages based on the user behaviour based on 90 days/3 months old data.
4. And we also support an extra condition based on how many times a given user did the above event with above event attribute conditions. Like say for the event <code>Logged In</code> you get get all the users who logged in <code>exactly 3 times</code> or <code>at least 3 times</code> or <code>at most 3 times</code></p>

<p>The earlier mongo implementation, all the events are stored in a single mongo <code>db</code> called <code>datapoints</code>. From the few amount of clients we had in the beginning, it is clear that the <code>events</code> people tracked are vastly different just based on the volume. Some events like <code>opened main page</code> will be in 10s of millions every day. And <code>product purchased</code> event would just be in thousands. And <code>product purchased</code> event might be of more interest when it came to segmentation queries. As users who did that event are much more important. So the volume of a given event and the significance of that event are both completely unrelated. An event of low volume might be of very high importance. And not just that, the developers might track every little interaction of the users. But the marketing folk might only be interested in 3-10 events.</p>

<p>Two major challenges.
1. Same app might track two different events that vary in volume, some events can be in 10&rsquo;s of millions while other events are just in 1000s
2. Even though an app might track 100s of events, there are only 3-10 events that the marketers are interested in.</p>

<p>So the first major diversion from the mongo implementation is having a separate db(index in elasticsearch terms) for each event, as search performance on one event shouldn&rsquo;t be dependent on all the rest of the events.</p>

<p>So all the <code>Added to Cart</code> events of app <code>Amazon</code> would be in the elasticsearch index <code>amazon-addedtocart</code>. This is the first major decision that we struck with for a long time, even though the two major challenges are not completely solved. It made things a lot easier. In the initial version of Elasticsearch implementation, we decided on having 2 shards per index, no matter the volume of the index. There is a concept of <code>index aliasing</code> in elasticsearch that helps in dealing with very high volume events. In elasticsearch the name of the index cant be changed once it is created, so are the number of shards. Index aliasing helps us with having more than one name/alias to query an index. A single elasticsearch index can have more than one alias. And more than one index can all have the same alias. So that when we query using an alias, all the indexes with that alias will be queried. But when you are inserting a object into an index, using an alias, that alias must point to only one index. So a <code>read alias</code> that you can use to query filters can point to more than one index. And a <code>write alias</code> must point to only one index for write operations to be successful. This concept of index aliasing helps us in dealing with really large indexes. As the number of shards per index is fixed after an index is created using aliasing you can direct new documents of a really big index to another document using <code>write alias</code> while searches point to both the old index and the new index.</p>

<p>In elasticsearch by default the all the fields are indexed, there is an extra field called <code>_all</code> that considers the entire document as a single field. Indexing happens according to the type of <code>analyser</code> you select. Our segmentation queries don&rsquo;t really require the default analyser that is provided. The analyser defines how the fields are tokenised and etc. So I had to disable analysis, so that the fields are considered as they are provided.</p>

<p>All the settings of number of shards, index aliases, and analysis settings for string fields are set using <code>index templates</code>. An index template can be used to set all types of settings based on the name of the index while it is being created. A sample index template might look like this.</p>

<script src="https://gist.github.com/syllogismos/824b069b4df415c3f2c3.js"></script>


<p>Using the above template we are doing
1. making default no of shards as 1 per index
2. creating an alias to all indexes, just by appending <code>-alias</code> to the end. So the index <code>amazon-addedtocart</code> can also be queried using <code>amazon-addedtocart-alias</code>. In the earlier implementation I didn&rsquo;t use aliases to the full potential, but I knew aliases will save by bum so I put an alias to every index that is created as above and it did help me to deal with very big indexes.
3. disable indexing <code>_all</code> filed, which we don&rsquo;t really use in segmentation.
4. change the default analyser to no analysis on all the string fields.</p>

<p>With the above template settings, I started a cluster with 4 nodes, each of 16GB memory, and out of the 4, 3 master eligible nodes. And started porting event data from mongo to elasticsearch using a open source tool called elaster. If I remember correctly, when we first moved to elasticsearch I was porting 60Million documents. On the other hand we were writing live tracking data from webworkers into elasticsearch.</p>

<h3>Query Language:</h3>

<h3>Stats and miscellaneous notes:</h3>

<ul>
<li>Using this architecture, I scaled the segmentation service from initial 60 million objects to almost 10-20Billion datapoints.</li>
<li>From mongo to initial 4 nodes of 16Gb cluster to a total of ~40-45 nodes of 32 Gb memory.</li>
<li>Total number of shards around ~20k</li>
<li>There is a concept of snapshots in elasticsearch, Along with this we also had a script that takes nightly backups of data and dumps it in S3. We also collect backups of the raw http api requests in s3, that goes in from kafka. But more backups never hurt anybody.</li>
<li><p>And we only supported 90 days of tracking data, so we had to delete data older than 90 days everyday. With data deletion also, there is a concept of <code>segments</code> on elasticsearch. Where objects reside on these segments. And when you delete an object it is just marked as delete and only when all the documents on a segment are deleted will they go away permanently. Luckily for us we delete data date wise, and the sharding also happens based on date of creation. Although all the documents deleted might not have gone as soon as they were deleted, they eventually permanently deleted.</p>

<h3>Issues:</h3></li>
<li><p>Type issues: One major problem we faced is, in elasticsearch once the type of a field is set to <code>int</code>, and if you are trying to insert any new document with that field containing a <code>string</code> it wont be inserted. This is a really huge gotcha, and if I was not careful I would be losing lots of event data. We always have to be so nice to the <code>users</code> no matter how dumb they seem to be, we just have to think of all the ways the services we are providing can be abused or misused. Say for example <code>cost</code> event attribute initially they are sending integers. And later on decided they send the same thing by prepending a string <code>$</code>. All the new datapoints will be rejected. And there are some event attributes that exist in all datapoints. If by mistake they get mistyped when the index created, every datapoint that comes next will get rejected. To deal with this, using index templates, I set the type of the known fields beforehand. To deal with this,I put the datapoints that failed to be inserted into es because of type issues, in a mongo, and the entire json object as one string in <code>error-amazon</code>(in this example) index in the same cluster. Usually these type errors luckily for us are caused in test apps, while people are testing the integrating. So I used to deal with these errors case by case basis. I had scripts ready that clean up the data and fix type errors and reindex in the proper event index. But in later versions of the segmentation we came up with a really nice solution that permanently solves this.</p></li>
<li><p>So from the above architecture, it is clear that there will be a separate index, for every new event the client starts to track. We are providing a single endpoint through the sdk, and the user might accidentally pass a variable in the event name field, like say the <code>user id</code>. With us not providing any limit on the number of the events they can track. This will end up creating tens of thousands of indexes, which will just bottle neck the entire cluster And it will become a disaster. And there are cases where some of our clients passed <code>user id</code> as the name of the event. I had to go modify the workers code and do a release at some ungodly hour to fix this.</p></li>
<li><p>When an index gets too big, bigger than the heap size of the node that particular shard is residing on. I had make use of the index aliasing operation to write to a newer index while searching on both the old and new indexes. This is also one of the major problems, I faced, and directly affected the stability of the cluster. In later versions of the segmentation, using the aliases, we automated whatever I was doing manually. And I will talk about these changes in later section.</p></li>
</ul>


<h2>User Segmentation filter:</h2>

<p>Eventually as we were getting more clients, doing the user segmentation filter queries on mongo db was also proving to be challenging. As we are querying on user db, the queries that hit the db are just normal filter queries. Apply all the filters and get a cursor, iterate through the cursor and return the list of users of a given segment. As with data point segmentation, we support all types to filter on the user db, <code>int</code>, <code>string</code>, <code>date</code>, <code>location</code>.</p>

<p>For mongo queries to work for user segmentation on all the user attributes that the app is tracking, we have a nested object that holds all the user attributes, and a single index on the the nested field. And as the user attributes are not fixed, and our clients can introduce anything they want to track. Having index such that queries work efficiently on all these fields is becoming challenging as the user dbs are getting bigger.</p>

<p>So eventually we wanted to move user segmentation also to elasticsearch. But the tricky part here is, that user objects receive updates on it, unlike datapoints. Such as, last seen of the user keeps updating every time the user uses it. Location of the user updates. This is fundamentally different from datapoints db. And the challenge now is if we should completely move away from mongo and switch to elasticsearch. But mongo infrastructure was heavily used by the push team to send push notifications once they are given the user ids. And we had to have mongo as the primary data source. And have elasticsearch that mirrors the data on mongo. We can&rsquo;t just write once and keep quite about it, we have to constantly update the elasticsearch data with new updates on user objects.</p>

<h3>River Hell:</h3>

<p>Above challenge brings us to a concept called <code>Rivers</code>, which exists to solve exactly this problem. The basic concept of rivers is that you have a primary db, some where like a mongodb/couchdb and etc. And you keep tailing all the CRUD operations that happen on the db and replay all those operations on elasticsearch. The river backend was provided by the elasticsearch backend in those days. And there exist several third party drivers that latch on to the CRUD operations that happen on the primary sources, like mongo/couchdb and etc.</p>

<p>Conceptually <code>Rivers</code> are what we needed. And I started the user elasticsearch cluster with some 6 nodes that serve user segmentation queries and started the rivers on all the bigger production clients. Things seemed to work well. Except RIVERS ARE UTTER CRAP AND VERY UNRELIABLE, and no wonder they were deprecated by elasticsearch guys as well.</p>

<p>They used to run fine, but after few days, the nodes used to max out on the jvm or memory and the entire user segmentation elasticsearch cluster used to not respond. I have to restart the questionable nodes, and after restarting the nodes, because of the replication shards, the user db indexes were back, but the individual river stopped and CRUD updates on the primary db stopped reflecting on the elasticsearch. So basically the user data on elasticsearch was stale. So I have to delete the index, restart the river from the beginning, and while this restarting is happening, I had to redirect the segmentation queries on to mongo db for backup. It was a mess. Rivers turned out to be such a pain to work with, it almost became a running joke in the entire office. I built a crude river dashboard that shows the status of the rivers and the difference in the number of users in elasticsearch index and the mongo db, this difference usually was a good indicator of the rivers status. And alerts that alert me when a river stops updating new users. Rivers are the most unreliable thing I had to use, and there is a good reason why it got deprecated.</p>

<h3>River Alternative:</h3>

<p>I was already looking at alternatives for rivers, and I know that things like <a href="https://github.com/mongodb-labs/mongo-connector">mongo connector</a> exist. But the question is how reliable that thing will be. But we got to know it was working well for similar use case from a asking around. Mongo already has a way of latching onto and tailing all the crud operations(oplog) and replaying all that. That is how the secondary dbs also replicate their data from the primary mongo nodes. And it is fairly robust. And the mongo connector provides us a way to make use of this replication and index into elasticsearch. Basically we are indexing stuff to elasticsearch(which it is supposed to be good at) and that stuff is basically obtained from oplog of the mongodb which is also fairly robustly implemented. So using mongo connector we are making use of things in which both mongo and elasticsearch are good at in their respective tasks.</p>

<p>By now I had a proper team mate and I made him fork mongo connector and replicate all the river functionality that we were using earlier and made the switch from rivers to mongo-connector.</p>

<h3>Stats:</h3>

<p>I think by then we were supporting user segmentation on around 80-100Million users. And after moving away from rivers and started using mongo connector, we were very comfortable with user segmentation. 100Million objects in user cluster seemed like nothing when compared to the datapoint cluster with almost 100x volume and aggregation queries. The only tricky part is not letting the data become too stale, and keep it up to date with mongo.</p>

<h2>All Users:</h2>

<p>All users filter is basically returning userids of a given app. The naive implementation is that getting a cursor with empty filter and then iterating it through each and every userid and returning them. But as the user db&rsquo;s got bigger and bigger this process took more and more time. <code>All users</code> query is fired not just when sending push campaign to all users. We also need it when we want a datapoint segment that goes like <code>All users who did not do this action in the last n days</code>. We do the normal datapoint segmentation and set difference it from <code>all users</code>.</p>

<p>Instead of querying all the user ids from the database all the time, you can store all the userids that were created till a particular time in  cache and query the rest from database, or update the cache to add newly added users hourly. But then for this to work we need an index on the <code>created_time</code> field of the user object. But we can make use of objectid property of mongo objects. It also contains the information related to when the object is created. Instead of creating an index on <code>created_time</code> field of the object we can filter directly on the object id to query on creation time of the object.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>from bson.objectid import ObjectId
</span><span class='line'>object_id_from_time_stamp = ObjectId.from_datetime(time_stamp)
</span><span class='line'>cursor = db.Users.find({'_id': {'$gt': object_id_from_time_stamp}})</span></code></pre></td></tr></table></div></figure>


<p>So I have a worker that updates the all users cache hourly to include the newly created users from that app. The cache key mechanism I used is such that I store 50k user ids in a single redis key and store the next 50k users in the next redis key. Also for smaller dbs I used to get all the users directly from db as often they are test apps, and it will be confusing for them to miss some users when they test using all users campaign.
I also store the last cache update time in redis, so that I can query all the new users created after that timestamp directly from db. I update the whole user cache weekly, just in case I miss some users some how.</p>

<p>This part of the code has to be implemented very carefully with lots of fall back mechanism. In further iterations I started getting user ids from the elasticsearch cluster. If Es fails for some reason, I get it from mongodb. And not all clients have ES river. And we a back up of querying es/mongo directly if the cache fails. The major reason I didn&rsquo;t store all the users in a single key is that If a single get request fails, the entire query returns zero users. Instead if I store only 50k users per key, even if single key might fail, the output wont be that unreasonable.</p>

<p>The sheer amount of edge cases that can screw up this result are so many I had to be very careful with this part.</p>

<p>But this improved the <code>All Users</code> query time from 10 minutes to in some cases less than 10 secs.</p>

<h2>Combining above filters.</h2>

<p>Now that we discussed all the three types of major filters,
* datapoint segmentation filter(D)
* user segmentation filter(U)
* all users query(A)</p>

<p>Each of them get the userids from three different sources. And a single segment is a combination of one or more of any of the above three filters. And the combination can either be <code>intersection</code> or <code>union</code>.</p>

<p>We compute all the filters separately and combine them using basic python operations.</p>

<p><code>Has not executed</code> is basically <code>All Users</code> - <code>has executed</code></p>

<p>There are lot of iterative improvements that can be made on this naive implementation. Each filter is computed synchronously.  This can be done asynchronously. Asynchronously computing more than one datapoint filter might not be a great idea, as it might stress the datapoint cluster even more. And if you observe carefully, if there are more than one user filter, they can all be made into a single query on the user db. All sorts of combinations can be tried.</p>

<p>And if you study more carefully you can actually make use of set theory to reduce the computation complexity and speed up segmentation even more.</p>

<p>Below <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code> are some combination of filters, and <code>S</code> is all users. So <code>S-C</code> is basically <code>has not executed</code> version of <code>C</code>.</p>

<p>Below is just one example of how to reduce the computation complexity of segmentation queries.</p>

<p><img src="http://i.imgur.com/DA6xihK.png" alt="Imgur" /></p>

<h2>Cluster config</h2>

<h2>backup</h2>

<p>We had backups in several stages, as soon as we get a http request, we back up that raw http request in s3, and then let web workers handle the request to be processed further.
At elasticsearch stage, we had two mechanisms, where we make use of snapshots, but this seemed to be unreliable. Every time a snapshot request is filed, it might or might not have worked. Given the sheer volume of the data we were dealing with, and our current index design, cluster operations like <code>index creation</code>, <code>index deleting</code>, <code>snapshot</code> these used to take some time. But major operations like search and indexing used to be very fast once an index got created. Anyway, snapshot is not really trustworthy in our case. So we just queried based on time stamp and had a worker back up the data from es to s3 in json format. I also had scripts ready so that I can recreate indexes from s3. But I always prayed we never had to use it.</p>

<h2>challenges while testing, smaller apps are different than bigger apps</h2>

<p>Testing used to be a little challenging, cause, in more than one case, smaller apps are treated slightly different than bigger apps that are at more than order or two orders of volume.</p>

<p>Say for example, <code>All Users</code> is implemented differently for smaller and bigger apps.</p>

<h2>Custom segments</h2>

<p>Instead of creating segments all the time, we also provide a way to store segmentation queries. So that they can create once and use them later.</p>

<p>Or a use case of where you are sending two push campaigns, and you don&rsquo;t want the users who were targeted in the first push campaign to get another push message, custom segments were very useful to exclude.</p>

<p>There are some custom segments that has more than 30 filters, its crazy.</p>

<h2>Overall diagrams/schemas</h2>

<ul>
<li><p>Overall schema of datapoints and user cluster write operations.
<img src="http://i.imgur.com/QWBjdt7.jpg" alt="Imgur" /></p>

<ol>
<li>raw http requests are handled by api/machines and backed up in s3 and sent to reports workers</li>
<li>report workers insert event data using bulk indexing into datapoint cluster</li>
<li>report workers also does some updates on mongo user objects</li>
<li>the datapoint indexing operations might fail, the failed docs will be going to a separate index on es and if that fails as well it goes to mongo.</li>
<li>datapoint cluster also gets event data from push workers, that captures campaign related information.</li>
<li>the mongo connector infra takes care of replicating user data from mongo to user elasticsearch.</li>
</ol>
</li>
<li><p>Overall schema of datapoints and user cluster search/read/segmentation operations.
<img src="http://i.imgur.com/kkLwq5b.jpg" alt="Imgur" /></p>

<ol>
<li>datapoint segmentation happens on datapoint cluster</li>
<li>user segmentation happens on user cluster, if river exists</li>
<li>if river doesn&rsquo;t exist it happens on mongo</li>
<li>there are some events like app_opened and other important events whose histograms based on time, might reveal important usage statistics and key metrics, like MAU, DAU and other such data. These histograms are obtained form datapoint cluster</li>
<li>campaign histograms, neat graphs based on campaign related events that shows how a campaign was delivered with time. you can get this data from datapoint cluster</li>
<li>acquisition stats from user cluster</li>
<li>key metrics such as uninstalled and etc can be obtained from user cluster</li>
<li>auto trigger campaigns need a simple search based on user id and event, and this is done on datapoint cluster.</li>
</ol>
</li>
</ul>


<h2>Challenges, elasticsearch quirks</h2>

<p>Once we replaced rivers with mongo-connector, user segmentation is smooth sailing when compared to datapoint segmentation. So most of the challenges I will be discussing in this section are datapoint cluster related and its design.</p>

<p>The single biggest challenge with datapoint cluster is how diverse the types of actions people are tracking, when it comes to volume. As I described earlier, we had one shard per index, as we are technically sharding on the name of the action. But we can&rsquo;t know what volume a given action will be before its created. The <code>read alias</code> and <code>write alias</code> of the created index sure does gives us some relief, but we are not exploiting it to the max. And some things are done manually which should have been automated. Like sometimes the size of an index gets so big, that I used to manually change the write alias to a new index, while searching on both the indexes.</p>

<p>Another challenge is that once a filed&rsquo;s type is induced by elasticsearch. And when you are trying to insert a doc with a different type, especially a filed that was induced to be <code>int</code> is getting documents with <code>string</code> it will throw up an error. 90% of the time, the mistyped data and the fields are not really of that much importance to marketers. But the rest of the 10% used to cause so many problems. I had a temporary fix to this, but we need to come up with a permanent solution to this.</p>

<p>The other request is to make the search queries case insensitive. Often the marketing people who are creating the segments don&rsquo;t know what exactly should be the search term. So they would prefer a case insensitive thing going. This request seems so innocent, but for me to implement this I have to rebuild all the indexes again with a different analyser. Given the sheer size of the data we are indexing daily, this  just seems like too much work for too little reward. I kept postponing this forever.</p>

<p>One more important thing is elasticsearch recommends a max heap size of only 32GB, and not more than that.</p>

<p>Given our index design, we are creating a new index every time they track a new event. There are no limitations on how many events an app might track. This resulted in having close to 10k shards per cluster. Which caused us so many problems. I don&rsquo;t even know if its normal, but somehow we made it work. Because of the sheer number of indexes/shards in the datapoint cluster, the meta data that the master nodes stores is so much, and most of the cluster operations used to take so much to succeed. And most of them fail. During really stressful periods of times, when big segmentation queries are running, and some new app is trying to start tracking new events. Most of them timed out cuz the index is not present. And the timed out documents are stored in a different mongo db, temporarily. When this happened, I got all the events they are tracking and keep on hitting the cluster with index creation requests till they succeed. I&rsquo;m slightly embarrassed we had to do this. It always caused us problems. And this is the main reason for us to rearchitect. And we are well prepared to do this as I was much more comfortable with es after this adventure, which lasted for almost 1.5 years. Now I had a proper team and I headed the re-architecture project before moving on from Moengage.</p>

<p>There are lot more stuff I&rsquo;m not able to recall right now, I might add more stuff later. probably not.</p>

<h2>Re-architecture</h2>

<h3>Index aliasing</h3>

<p>As described in earlier sections, the biggest challenge with the design is the sheer number of shards that exist in the cluster, no matter if a given index and its respective event gets queried a lot. A given app might be tracking a really high volume action, and at the same time several number of low volume actions that need their own events based on the above design. Somehow if we make use of index aliasing to be clever with how we direct what event data is written in what indexes, and what indexes will be searched given a segmentation query on the action. More or less the goal of the re-architecture is to reduce the number of shards in the cluster and more or less have all the shards of similar size.</p>

<p>For this we will have have one master index that contains all the smaller event data, basically when a new event gets tracked it is directed into the master index, by adding a new alias. As the days go along and that event starts getting more and more data, based on the volume of the event, we start writing the data into separate index. Based on the volume of the data, we decide if that new index gets rotated daily/weekly/monthly.</p>

<p>We will be having a external worker that constantly monitors what the volume of each event is and decides its <code>read alias</code> and <code>write alias</code>. And all the search queries will be on the event&rsquo;s <code>read alias</code> and all the insert operations will be on the event&rsquo;s <code>write alias</code>.</p>

<p>This will bring down the number of indexes that exist on the cluster a lot. and put us more in control.</p>

<p>This might reduce the number of indices by lot, but the master index we were talking about will have thousands of aliases.</p>

<h3>Case Insensitivity.</h3>

<p>This request I&rsquo;m dodging for a long time is now feasible as we were building the cluster again from the ground up and moving from the old cluster.</p>

<p>Basically we just have to change the analyser we were using to analyse our string fields.</p>

<p>The case insensitive analyser basically while indexes with lower case of all the string fields. If we index with lower case no matter what, and search with lower case, its just equivalent to case insensitivity.</p>

<p>We introduce this case insensitivity analyser using the index templates. For backward compatibility we store the unanalysed field as well as case insensitive analyser filed by having the template like below.</p>

<p>We access the old field while searching by querying on <code>field</code> name, and new case insensitive field with <code>field.case_insensitive</code></p>

<script src="https://gist.github.com/syllogismos/d7e63fd683484ab2d635b4c735bb7bfb.js"></script>


<h3>Type errors/mismatch</h3>

<p>Another major problem is that once you create an index, elasticsearch will induce the type of the fields as they come along. But once a document with a wrong datatype that can&rsquo;t be type casted is being inserted it will throw up an error. One elegant solution is that have your own type inference engine, and insert a field of type <code>int</code> and keyword <code>age</code> into <code>age_int</code> field, and when <code>age</code> of type <code>string</code> comes along, you can insert into <code>age_string</code>. If you infer the type of the field before you can handle this elegantly. But then inference of type is in your hands now.. And you have to modify the queries on the application side to take this into consideration. This way we wont face this problem anymore.</p>

<h3>Suggestions:</h3>

<p>When people are creating segments on the dashboard, often people wont know what goes into it unless they go into another dashboard with all types of values a given key will be taking, So having a top 5 or top 10 values a particular string field takes will help a lot from user interface pov. Elasticsearch does have suggestions api, but I cant remember correctly, how we making use of it is non trivial or downright impossible with current index template settings and analyser. But in one of the hackathons I implemented this with some caching mechanism, from a simple aggregation query I can get the top 10 values a given field takes. And I maintain a cache that gets updated weekly or daily that gives the top 10 values.</p>

<h3>event management dashboard</h3>

<p>As we discussed earlier, there is a huge disconnect between the developers who decide what to track, and the marketers who end up creating push campaigns and knows what events are relevant. As the size and the user base of the client integrating increases, the disconnect increases. As both are from two different departments.</p>

<p>Usually what developers do is track comprehensively and let marketers create campaigns on what ever they are interested in. Funnily because of this some clients happen to track more than 1000 events, with a dropdown of more than 1000 items in the dashboard. Which is almost nonsensical, especially when they are interested in at max 15 events.</p>

<p>If there are only 15 events marketers are interested in and we are indexing 1000s of useless indices, there is no point indexing everything in elasticsearch which is not cheap to have.</p>

<p>So to solve this if we proved a dashboard where they can decide what action they are segmentable. And we will only be indexing what ever they are interested in and chuck the rest in S3, and if they decide they want it back, start indexing it again and load the last 90days data from s3.</p>

<h1>Miscellaneous notes and scripts.</h1>

<p>Be comfortable with the rest apis, I found it much more helpful than any elasticsearch plugin. Cat apis and the rest apis using <code>curl</code> and <code>jq</code> made my life so simple.</p>

<p>Learn the query language, once you are comfortable with it, it wont be as scary as it looks the first time you go see the documentation. <code>must</code>, <code>must_not</code>, <code>bool</code>, nested aggregations, histograms and etc.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1. Routing settings
</span><span class='line'>
</span><span class='line'>curl -XPUT 172.31.46.49:9200/_cluster/settings -d '{
</span><span class='line'>"persistent" : {
</span><span class='line'>"cluster.routing.allocation.node_initial_primaries_recoveries": 4,
</span><span class='line'>"cluster.routing.allocation.node_concurrent_recoveries": 15,
</span><span class='line'>"indices.recovery.max_bytes_per_sec": "120mb",
</span><span class='line'>"indices.recovery.concurrent_streams": 6
</span><span class='line'>}
</span><span class='line'>}'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>'{
</span><span class='line'>"persistent": {
</span><span class='line'>"cluster.routing.allocation.node_concurrent_rebalance": 2}}'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>"cluster.routing.allocation.node_initial_primaries_recoveries": 4
</span><span class='line'>"cluster.routing.allocation.node_concurrent_recoveries": 15
</span><span class='line'>"indices.recovery.max_bytes_per_sec": 100mb
</span><span class='line'>"indices.recovery.concurrent_streams": 5
</span><span class='line'>
</span><span class='line'>curl -XPOST 172.31.46.49:9200/_cluster/reroute -d '{"commands":[{"move":{"index":"wynkmusic-click", "shard":0, "from_node":"30GB_1TB_ComputeNode9", "to_node":"30GB_1TB_ComputeNode8"}}]}'
</span><span class='line'>
</span><span class='line'>###################################################################
</span><span class='line'>
</span><span class='line'>curl -XPOST 172.31.46.49:9200/_cluster/reroute -d '{
</span><span class='line'>  "commands": [
</span><span class='line'>    {
</span><span class='line'>      "move": {
</span><span class='line'>        "index": "wynkmusic-songplayedlong-feb0105",
</span><span class='line'>        "shard": 0,
</span><span class='line'>        "from_node": "30GB_1TB_ComputeNode7",
</span><span class='line'>        "to_node": "30GB_1TB_ComputeNode9"
</span><span class='line'>      }
</span><span class='line'>    },
</span><span class='line'>    {
</span><span class='line'>      "move": {
</span><span class='line'>        "index": "wynkmusic-songplayedlong-jan2531",
</span><span class='line'>        "shard": 0,
</span><span class='line'>        "from_node": "30GB_1TB_ComputeNode7",
</span><span class='line'>        "to_node": "30GB_1TB_ComputeNode8"
</span><span class='line'>      }
</span><span class='line'>    }
</span><span class='line'>  ]
</span><span class='line'>}'
</span><span class='line'>##################################################################
</span><span class='line'>
</span><span class='line'>2. Rerouting a shard manually
</span><span class='line'>
</span><span class='line'>curl -XPOST 172.31.46.49:9200/_cluster/reroute -d '{
</span><span class='line'>        "commands" : [ {
</span><span class='line'>              "allocate" : {
</span><span class='line'>                  "index" : "shopotest-acceptonlinedisclaimer",
</span><span class='line'>                  "shard" : 0, 
</span><span class='line'>                  "node" : "30GB_1TB_ComputeNode5"
</span><span class='line'>              }
</span><span class='line'>            }
</span><span class='line'>        ]
</span><span class='line'>    }' | jq '.' &gt; reroute_gos.json
</span><span class='line'>
</span><span class='line'>###################################################################
</span><span class='line'>
</span><span class='line'>3. Emptying a node completely, you can do this before decommissioning a node.
</span><span class='line'>
</span><span class='line'>curl -XPUT 172.31.33.23:9200/_cluster/settings -d '{
</span><span class='line'>"transient": {
</span><span class='line'>"cluster.routing.allocation.exclude._ip": "172.31.33.23"
</span><span class='line'>}}' | jq '.'
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>curl -XPUT 172.31.46.49:9200/_cluster/settings -d '{
</span><span class='line'>"transient": {
</span><span class='line'>"cluster.routing.allocation.exclude._ip": "172.31.41.73"
</span><span class='line'>}}' | jq '.'
</span></code></pre></td></tr></table></div></figure>


<h1>Conclusion</h1>

<p>Here is a brief explanation of how I made segmentation work for moengage using elasticsearch. I had a lot of fun with this challenge with no experience in distributed computing, I had to ramp up very fast and learned a lot. Being part of a startup forced me to do lot of learning in a very short period of time not just tech related, learned about building an engineering team, release process, workers, distributed computing, and what not to do. When I first joined I didn&rsquo;t think I get to play a major role on the moengage tech stack. For a long time till I left I took care of the elasticsearch infrastructure, application side logic, architecture design and etc all by myself with a engineering bandwidth. I&rsquo;m very grateful for having this opportunity. I already postponed this blog post for a long time. I would have liked it to be much more comprehensive with code examples and so on. After a long of break from elasticsearch, I had to write this from memory. And lastly I&rsquo;m forever grateful for moengage for pushing me so much and making me deliver.</p>

<p>Fun email we got on new years eve from arguably the biggest client we had in the initial stages.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Hi Guys
</span><span class='line'> 
</span><span class='line'>I have been trying for a very long time and no matter what combination of filters are tried the server response is error. We are unable to make very basic segment of customers. Frankly on New Years Eve we are not able to reach the right customer, this is disappointing . Attaching screen shot here.</span></code></pre></td></tr></table></div></figure>


<p>This is when we moved the segmentation architecture from mongo to elasticsearch. Everything was ready, only the data porting from mongo to es was left and releasing the new segmentation code. So in 24 hours we released the new segmentation logic and ported the data. The only reason I didn&rsquo;t release was because of me being relatively new in the company and me having to check everything twice or thrice. This email is the push we needed to jump head first. And it worked for a long time. Fun times.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2017/01/02/santander-product-recommendation-kaggle/">Santander Product Recommendation Kaggle</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-01-02T22:45:43+05:30'><span class='date'><span class='date-month'>Jan</span> <span class='date-day'>2</span><span class='date-suffix'>nd</span>, <span class='date-year'>2017</span></span> <span class='time'>10:45 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="https://www.kaggle.com/c/santander-product-recommendation">Santander Product Recommendation</a></p>

<p>Under their current system, a small number of Santander’s customers receive many
recommendations while many others rarely see any resulting in an uneven customer
experience. In their second competition, Santander is challenging Kagglers to
predict which products their existing customers will use in the next month based
on their past behavior and that of similar customers.</p>

<p>With a more effective recommendation system in place, Santander can better meet
the individual needs of all customers and ensure their satisfaction no matter
where they are in life.</p>

<p>Based on Users history and previous products they subscribed to,
predict what products he will be interested in future.</p>

<h1>Data</h1>

<p>In this competition, you are provided with 1.5 years of customers behavior data
from Santander bank to predict what new products customers will purchase.
The data starts at 2015-01-28 and has monthly records of products a customer has,
such as &ldquo;credit card&rdquo;, &ldquo;savings account&rdquo;, etc. You will predict what additional
products a customer will get in the last month, 2016-06-28, in addition to what
they already have at 2016-05-28. These products are the columns named: ind<em>(xyz)</em>ult1,
which are the columns #25 - #48 in the training data. You will predict what a
customer will buy in addition to what they already had at 2016-05-28.</p>

<p>The test and train sets are split by time, and public and private leaderboard
sets are split randomly.</p>

<table>
<thead>
<tr>
<th>Id </th>
<th> Column Name </th>
<th>  Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 </td>
<td> fecha_dato </td>
<td>    The table is partitioned for this column</td>
</tr>
<tr>
<td>2 </td>
<td> ncodpers </td>
<td>  Customer code</td>
</tr>
<tr>
<td>3 </td>
<td> ind_empleado </td>
<td>  Employee index: A active, B ex employed, F filial, N not employee, P pasive</td>
</tr>
<tr>
<td>4 </td>
<td> pais_residencia </td>
<td>   Customer&rsquo;s Country residence</td>
</tr>
<tr>
<td>5 </td>
<td> sexo </td>
<td>  Customer&rsquo;s sex</td>
</tr>
<tr>
<td>6 </td>
<td> age </td>
<td>   Age</td>
</tr>
<tr>
<td>7 </td>
<td> fecha_alta </td>
<td>    The date in which the customer became as the first holder of a contract in the bank</td>
</tr>
<tr>
<td>8 </td>
<td> ind_nuevo </td>
<td> New customer Index. 1 if the customer registered in the last 6 months.</td>
</tr>
<tr>
<td>9 </td>
<td> antiguedad </td>
<td>    Customer seniority (in months)</td>
</tr>
<tr>
<td>10 </td>
<td> indrel </td>
<td>   1 (First/Primary), 99 (Primary customer during the month but not at the end of the month)</td>
</tr>
<tr>
<td>11 </td>
<td> ult_fec_cli_1t </td>
<td>   Last date as primary customer (if he isn&rsquo;t at the end of the month)</td>
</tr>
<tr>
<td>12 </td>
<td> indrel_1mes </td>
<td>  Customer type at the beginning of the month ,1 (First/Primary customer), 2 (co-owner ),P (Potential),3 (former primary), 4(former co-owner)</td>
</tr>
<tr>
<td>13 </td>
<td> tiprel_1mes </td>
<td>  Customer relation type at the beginning of the month, A (active), I (inactive), P (former customer),R (Potential)</td>
</tr>
<tr>
<td>14 </td>
<td> indresi </td>
<td>  Residence index (S (Yes) or N (No) if the residence country is the same than the bank country)</td>
</tr>
<tr>
<td>15 </td>
<td> indext </td>
<td>   Foreigner index (S (Yes) or N (No) if the customer&rsquo;s birth country is different than the bank country)</td>
</tr>
<tr>
<td>16 </td>
<td> conyuemp </td>
<td> Spouse index. 1 if the customer is spouse of an employee</td>
</tr>
<tr>
<td>17 </td>
<td> canal_entrada </td>
<td>    channel used by the customer to join</td>
</tr>
<tr>
<td>18 </td>
<td> indfall </td>
<td>  Deceased index. N/S</td>
</tr>
<tr>
<td>19 </td>
<td> tipodom </td>
<td>  Addres type. 1, primary address</td>
</tr>
<tr>
<td>20 </td>
<td> cod_prov </td>
<td> Province code (customer&rsquo;s address)</td>
</tr>
<tr>
<td>21 </td>
<td> nomprov </td>
<td>  Province name</td>
</tr>
<tr>
<td>22 </td>
<td> ind_actividad_cliente </td>
<td>    Activity index (1, active customer; 0, inactive customer)</td>
</tr>
<tr>
<td>23 </td>
<td> renta </td>
<td>    Gross income of the household</td>
</tr>
<tr>
<td>24 </td>
<td> segmento </td>
<td> segmentation: 01 - VIP, 02 - Individuals 03 - college graduated</td>
</tr>
<tr>
<td>25 </td>
<td> ind_ahor_fin_ult1 </td>
<td>    Saving Account</td>
</tr>
<tr>
<td>26 </td>
<td> ind_aval_fin_ult1 </td>
<td>    Guarantees</td>
</tr>
<tr>
<td>27 </td>
<td> ind_cco_fin_ult1 </td>
<td> Current Accounts</td>
</tr>
<tr>
<td>28 </td>
<td> ind_cder_fin_ult1 </td>
<td>    Derivada Account</td>
</tr>
<tr>
<td>29 </td>
<td> ind_cno_fin_ult1 </td>
<td> Payroll Account</td>
</tr>
<tr>
<td>30 </td>
<td> ind_ctju_fin_ult1 </td>
<td>    Junior Account</td>
</tr>
<tr>
<td>31 </td>
<td> ind_ctma_fin_ult1 </td>
<td>    Más particular Account</td>
</tr>
<tr>
<td>32 </td>
<td> ind_ctop_fin_ult1 </td>
<td>    particular Account</td>
</tr>
<tr>
<td>33 </td>
<td> ind_ctpp_fin_ult1 </td>
<td>    particular Plus Account</td>
</tr>
<tr>
<td>34 </td>
<td> ind_deco_fin_ult1 </td>
<td>    Short-term deposits</td>
</tr>
<tr>
<td>35 </td>
<td> ind_deme_fin_ult1 </td>
<td>    Medium-term deposits</td>
</tr>
<tr>
<td>36 </td>
<td> ind_dela_fin_ult1 </td>
<td>    Long-term deposits</td>
</tr>
<tr>
<td>37 </td>
<td> ind_ecue_fin_ult1 </td>
<td>    e-account</td>
</tr>
<tr>
<td>38 </td>
<td> ind_fond_fin_ult1 </td>
<td>    Funds</td>
</tr>
<tr>
<td>39 </td>
<td> ind_hip_fin_ult1  </td>
<td>    Mortgage</td>
</tr>
<tr>
<td>40 </td>
<td> ind_plan_fin_ult1 </td>
<td>    Pensions</td>
</tr>
<tr>
<td>41 </td>
<td> ind_pres_fin_ult1 </td>
<td>    Loans</td>
</tr>
<tr>
<td>42 </td>
<td> ind_reca_fin_ult1 </td>
<td>    Taxes</td>
</tr>
<tr>
<td>43 </td>
<td> ind_tjcr_fin_ult1 </td>
<td>    Credit Card</td>
</tr>
<tr>
<td>44 </td>
<td> ind_valo_fin_ult1 </td>
<td>    Securities</td>
</tr>
<tr>
<td>45 </td>
<td> ind_viv_fin_ult1 </td>
<td> Home Account</td>
</tr>
<tr>
<td>46 </td>
<td> ind_nomina_ult1 </td>
<td>  Payroll</td>
</tr>
<tr>
<td>47 </td>
<td> ind_nom_pens_ult1 </td>
<td>    Pensions</td>
</tr>
<tr>
<td>48 </td>
<td> ind_recibo_ult1 </td>
<td>  Direct Debit</td>
</tr>
</tbody>
</table>


<hr />

<h1>My approach</h1>

<p>Best Private score: 0.030378(Rank 120/1785) {Notebook 11. and 12.}</p>

<p>Divided data month wise for so that it will be easier for my local computer to handle.
So a simple month based grep will create files <code>train_2015_01_28.csv</code> that contain data
from Jan 2015 and so on.</p>

<p>And for each month, computed what users added in then next month, say for example for
the month June 2015, there are total 631957 train data rows, but the number of products
added in that month were 41746 by all the users combined. This data will be in files
<code>added_product_2015_06_28.csv</code>. I precomputed all these so that I don&rsquo;t have to do it again
and again for each model. For each month we will be training on the users that added a
new product the next month. That makes the data even more maneagable in terms of size.
Foreach month there are on an average of 35-40k users who added a new product.
We are interested how likely a user is interested in a new product.
Thanks to <code>BreakfastPirate</code> for making the train data an order of magnitude lesser by
showing in the forums this approach gives us meaningful results. The computation of
<code>added_product_*</code> files can be found in <code>999. Less is more.ipynb</code> notebook.</p>

<h2>Feature Engineering.</h2>

<p>So just to reiterate, the final training will be done on, for each month, we get all the users
who added a product in the next month, which reduces the train size by 10x, and combine this data
for all the months.</p>

<p>And data clean up and imputation is done by assuming for categorical variables the median values,
and for varibles like rent, mean based on the city of the user. This job was made a lot easier
because of <code>Alan (AJ) Pryor, Jr.</code>&rsquo;s script that cleans up the data and does imputation.</p>

<p>Lag features from the last 4 months. Along with the raw lags of the product subscription
history of a user, I computed 6 more features based on the past 4 month history of
product subscription that goes as below.</p>

<ul>
<li>product exists atleast once in the past</li>
<li>product exists all the months(last 4 months)</li>
<li>product doesn&rsquo;t exist at all</li>
<li>product removed in the past(removed before this current month)</li>
<li>product added in the past(added before this current month)</li>
<li>product removed recently(removed this current month)</li>
<li>product added recently(added this current month)</li>
</ul>


<p>These are the features that gave me the best results. Trained using xgboost.
and you can find them in notebook <code>11.</code> and best hyperparameters through
grid search in notebook <code>12.</code></p>

<p><strong>Best Hyperparameters through grid search</strong>:<br/>
<code>num_class</code>: 24,<br/>
<code>silent</code>: 0, <br/>
<code>eval_metric</code>: &lsquo;mlogloss&rsquo;, <br/>
<code>colsample_bylevel</code>: 0.95, <br/>
<code>max_delta_step</code>: 7,<br/>
<code>min_child_weight</code>: 1,<br/>
<code>subsample</code>: 0.9,<br/>
<code>eta</code>: 0.05,<br/>
<code>objective</code>: &lsquo;multi:softprob&rsquo;,<br/>
<code>colsample_bytree</code>: 0.9,<br/>
<code>seed</code>: 1428,<br/>
<code>max_depth</code>: 6</p>

<h2>Results:</h2>

<p>Notebooks convention is that the number of notebook will correspond to the notebook
that start with that file, and the decimal number will also be an extra submission in
the same notebook. So, you can find all 18, 18.4, 18.2 in the notebook that starts
with 18. in my notebooks. And all the notebooks contain the results, graphs and etc.</p>

<p>And notebooks that start with 999. are helper scripts and etc.</p>

<h3>Leaderboard scores of my various approaches:</h3>

<p>Just included the submission that scored more than 0.03 in private leaderboard</p>

<table>
<thead>
<tr>
<th>Notebook </th>
<th> Public Score </th>
<th> Private Score </th>
<th> Private Rank/1785</th>
</tr>
</thead>
<tbody>
<tr>
<td>12.</td>
<td>0.0300179</td>
<td>0.030378</td>
<td>120</td>
</tr>
<tr>
<td>11</td>
<td>0.0300507</td>
<td>0.0303626</td>
<td>129</td>
</tr>
<tr>
<td>14.2</td>
<td>0.0300296</td>
<td>0.0303479</td>
<td> -</td>
</tr>
<tr>
<td>11.1</td>
<td>0.0300342</td>
<td>0.0303458</td>
<td> -</td>
</tr>
<tr>
<td>18.4</td>
<td>0.030012</td>
<td>0.0303328</td>
<td>-</td>
</tr>
<tr>
<td>18.3</td>
<td>0.0299928</td>
<td>0.0303158</td>
<td>-</td>
</tr>
<tr>
<td>14.3</td>
<td>0.0299976</td>
<td>0.0303028</td>
<td>-</td>
</tr>
<tr>
<td>18.2</td>
<td>0.0300021</td>
<td>0.0302888</td>
<td>-</td>
</tr>
<tr>
<td>9.2</td>
<td>0.0299777</td>
<td>0.0302416</td>
<td>-</td>
</tr>
<tr>
<td>17</td>
<td>0.0299372</td>
<td>0.0302239</td>
<td>-</td>
</tr>
<tr>
<td>9</td>
<td>0.0299042</td>
<td>0.0301812</td>
<td>-</td>
</tr>
<tr>
<td>16</td>
<td>0.029886</td>
<td>0.0301792</td>
<td>-</td>
</tr>
<tr>
<td>18.1</td>
<td>0.0298155</td>
<td>0.0301786</td>
<td>-</td>
</tr>
<tr>
<td>16.3</td>
<td>0.0297175</td>
<td>0.030066</td>
<td>-</td>
</tr>
<tr>
<td>17.1</td>
<td>0.0297196</td>
<td>0.0300583</td>
<td>-</td>
</tr>
</tbody>
</table>


<h3>Map 7 score for each iteration of xgboost of my best submission:</h3>

<p><img src="http://i.imgur.com/FkB1alH.png" alt="Imgur" /></p>

<h3>Feature importance in my best submission:</h3>

<p><img src="http://i.imgur.com/6FpOBkG.png" alt="Imgur" /></p>

<h2>Additional appraoches that I tried.</h2>

<p>I looked at product histories of several users over months, to understand when,
a product is more likely to be added, to help me with my intuitions, and add more features.
Below are product histories of few users.</p>

<p><img src="http://i.imgur.com/HkqgN5b.png" alt="Imgur" /><br/>
<img src="http://i.imgur.com/DQXJYlq.png" alt="Imgur" /><br/>
<img src="http://i.imgur.com/KWhauNs.png" alt="Imgur" /><br/>
<img src="http://i.imgur.com/Cwlwzi6.png" alt="Imgur" /><br/>
<img src="http://i.imgur.com/KRZvLhS.png" alt="Imgur" /></p>

<p>Below is a representation of how similar each product is to other products if
each product is defined as a set of users who subscribed to that particular product.</p>

<p><strong>Cosine Similarities of products</strong><br/>
<img src="http://i.imgur.com/hBDBkn6.png" alt="Imgur" /></p>

<p><strong>Jacobian Similarities of products</strong><br/>
<img src="http://i.imgur.com/LVXPVhL.png" alt="Imgur" /></p>

<p>There are two important things from the above graphs, that I wanted to capture in terms
of features.</p>

<ul>
<li>From the product history vizs, if a particular product is being added and removed
consitently and if it doesn&rsquo;t exist it is more likely to be added.. So features like,
is_recently_added, is_recently_removed, exists_in_the_past, no_of_times_product_flanked
no_positive_flanks, no_negative_flanks and etc.. In my training set, I only considered
the past 4 months product subscription history, but from one of the top solution
sharing posts I noticed that people used entire product history to generate these features.
That might have increased my score just considering the entire history for each month
to generate these features.</li>
<li>Another thing I wanted to capture is how likely is a product to be subscribed based
on other products that were recently added. Say from the similarity vizs you can see that
how closely <code>cno_fin</code> is correlated to <code>nomina</code> or <code>nom_pens</code> and from some product history
vizs I observed that if <code>cno_fin</code> was added recently even though <code>nomina</code> never had a history
for a given user, he is likely to add it next month. So additional features I generated
are based on current months subscription data, from jacobian similarity weights and cosine
similarity weights I simply summed the weights of unsubscribed products of the respective
subscirbed products. These ended up being valuable features, with hight feature importance
scores but I didn&rsquo;t find them add more to my lb score.
<img src="http://i.imgur.com/cQGfa0p.png" alt="Imgur" /></li>
<li>Additional features I tried are lag features related to user attributes, but I didn&rsquo;t find these
added much to my lb scores. Say a user changed from non primary subscriber to a primary
subscriber, and he might be intersted in or be eligible for more products..</li>
<li>I wanted to caputure the trends and seasonality of product subscriptions, so along with raw month
features, as jan is closer to december then what 1, 12 represents so instead use <code>np.cos</code>, <code>np.sin</code>
of the month numbers. We can also use a period of 3 months by just using features
<code>np.cos(month/4)</code> and <code>np.sin(month/4)</code></li>
</ul>


<h2>Things learned from post competition solution sharing.</h2>

<ul>
<li>Entire product history is much more use ful than limiting myself to just 4 past months</li>
<li>Likilyhood of a product getting subscribed is also dependent on the month. I was not able to
successfully exploit this.</li>
</ul>


<p>Still reading various solutions, will update them once I get done with them.. Here are the direct links<br/>
* <a href="https://www.kaggle.com/c/santander-product-recommendation/forums/t/26835/1-solution">1st place solution</a>
* <a href="https://www.kaggle.com/c/santander-product-recommendation/forums/t/26824/2nd-place-solution">2nd place solution</a>
* <a href="https://www.kaggle.com/c/santander-product-recommendation/forums/t/26899/3rd-place-solution-with-code">3rd place solution</a></p>

<hr />

<h1>Premilinary data analysis</h1>

<h2>2. ncodpers</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 2 | sort -d | uniq -c | wc -l
</span><span class='line'>  956646</span></code></pre></td></tr></table></div></figure>


<h2>3. ind_empleado</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 3 | sort -d | uniq -c   
</span><span class='line'>27734
</span><span class='line'>2492 A
</span><span class='line'>3566 B
</span><span class='line'>2523 F
</span><span class='line'>13610977 N
</span><span class='line'>  17 S
</span><span class='line'>   1 "ind_empleado"  </span></code></pre></td></tr></table></div></figure>


<h2>4. pais_residencia</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 4 | sort -d | uniq -c
</span><span class='line'>27734  
</span><span class='line'> 111 AD  
</span><span class='line'> 221 AE  
</span><span class='line'>  17 AL  
</span><span class='line'>  68 AO  
</span><span class='line'>4835 AR  
</span><span class='line'> 476 AT  
</span><span class='line'> 424 AU  
</span><span class='line'>  34 BA  
</span><span class='line'>1526 BE  
</span><span class='line'> 476 BG  
</span><span class='line'>   6 BM  
</span><span class='line'>1514 BO  
</span><span class='line'>2351 BR  
</span><span class='line'> 102 BY  
</span><span class='line'>  17 BZ  
</span><span class='line'> 446 CA  
</span><span class='line'>  17 CD  
</span><span class='line'>  17 CF  
</span><span class='line'>  34 CG  
</span><span class='line'>1995 CH  
</span><span class='line'>  51 CI  
</span><span class='line'> 989 CL  
</span><span class='line'>  85 CM  
</span><span class='line'> 563 CN  
</span><span class='line'>3526 CO  
</span><span class='line'> 147 CR  
</span><span class='line'> 758 CU  
</span><span class='line'> 102 CZ  
</span><span class='line'>4625 DE  
</span><span class='line'>  11 DJ  
</span><span class='line'> 226 DK  
</span><span class='line'> 424 DO  
</span><span class='line'>  86 DZ  
</span><span class='line'>2169 EC  
</span><span class='line'>  45 EE  
</span><span class='line'>  68 EG  
</span><span class='line'>13553710 ES  
</span><span class='line'>  34 ET  
</span><span class='line'> 345 FI  
</span><span class='line'>5161 FR  
</span><span class='line'>  51 GA  
</span><span class='line'>4605 GB  
</span><span class='line'>  17 GE  
</span><span class='line'>  17 GH  
</span><span class='line'>  17 GI  
</span><span class='line'>  17 GM  
</span><span class='line'>  51 GN  
</span><span class='line'> 119 GQ  
</span><span class='line'> 243 GR  
</span><span class='line'> 130 GT  
</span><span class='line'>  34 GW  
</span><span class='line'>  51 HK  
</span><span class='line'> 282 HN  
</span><span class='line'>  68 HR  
</span><span class='line'>  37 HU  
</span><span class='line'> 409 IE  
</span><span class='line'> 413 IL  
</span><span class='line'> 187 IN  
</span><span class='line'>  17 IS  
</span><span class='line'>2947 IT  
</span><span class='line'>  11 JM  
</span><span class='line'> 239 JP  
</span><span class='line'>  72 KE  
</span><span class='line'>  17 KH  
</span><span class='line'>  96 KR  
</span><span class='line'>  17 KW  
</span><span class='line'>  17 KZ  
</span><span class='line'>  17 LB  
</span><span class='line'>  45 LT  
</span><span class='line'> 124 LU  
</span><span class='line'>  17 LV  
</span><span class='line'>  17 LY  
</span><span class='line'> 396 MA  
</span><span class='line'>  96 MD  
</span><span class='line'>  51 MK  
</span><span class='line'>  17 ML  
</span><span class='line'>  17 MM  
</span><span class='line'>  51 MR  
</span><span class='line'>   2 MT  
</span><span class='line'>2573 MX  
</span><span class='line'>  34 MZ  
</span><span class='line'> 214 NG  
</span><span class='line'>  62 NI  
</span><span class='line'> 757 NL  
</span><span class='line'> 136 NO  
</span><span class='line'>  51 NZ  
</span><span class='line'>  22 OM  
</span><span class='line'>  77 PA  
</span><span class='line'> 900 PE  
</span><span class='line'>  34 PH  
</span><span class='line'>  85 PK  
</span><span class='line'> 599 PL  
</span><span class='line'> 101 PR  
</span><span class='line'>1419 PT  
</span><span class='line'>1430 PY  
</span><span class='line'>  52 QA  
</span><span class='line'>2931 RO  
</span><span class='line'>  34 RS  
</span><span class='line'> 769 RU  
</span><span class='line'>  79 SA  
</span><span class='line'> 603 SE  
</span><span class='line'> 117 SG  
</span><span class='line'>  85 SK  
</span><span class='line'>  17 SL  
</span><span class='line'>  68 SN  
</span><span class='line'> 102 SV  
</span><span class='line'>  17 TG  
</span><span class='line'> 102 TH  
</span><span class='line'>  17 TN  
</span><span class='line'>  62 TR  
</span><span class='line'>  34 TW  
</span><span class='line'> 493 UA  
</span><span class='line'>3651 US  
</span><span class='line'> 510 UY  
</span><span class='line'>2331 VE  
</span><span class='line'>  34 VN  
</span><span class='line'> 119 ZA  
</span><span class='line'>  11 ZW  
</span><span class='line'>   1 "pais_residencia"</span></code></pre></td></tr></table></div></figure>


<h2>5. sexo</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 5 | sort -d | uniq -c
</span><span class='line'>27804
</span><span class='line'>6195253 H
</span><span class='line'>7424252 V
</span><span class='line'>   1 "sexo"</span></code></pre></td></tr></table></div></figure>


<h2>6. age</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 6 | sort -d | uniq -c
</span><span class='line'> 733   2
</span><span class='line'>1534   3
</span><span class='line'>2210   4
</span><span class='line'>3004   5
</span><span class='line'>3673   6
</span><span class='line'>3792   7
</span><span class='line'>4744   8
</span><span class='line'>5887   9
</span><span class='line'>7950  10
</span><span class='line'>10481  11
</span><span class='line'>12546  12
</span><span class='line'>12745  13
</span><span class='line'>12667  14
</span><span class='line'>13118  15
</span><span class='line'>11759  16
</span><span class='line'>11953  17
</span><span class='line'>10989  18
</span><span class='line'>21597  19
</span><span class='line'>422867  20
</span><span class='line'>675988  21
</span><span class='line'>736314  22
</span><span class='line'>779884  23
</span><span class='line'>734785  24
</span><span class='line'>472016  25
</span><span class='line'>347778  26
</span><span class='line'>281981  27
</span><span class='line'>240192  28
</span><span class='line'>205709  29
</span><span class='line'>186040  30
</span><span class='line'>167985  31
</span><span class='line'>169537  32
</span><span class='line'>170477  33
</span><span class='line'>174574  34
</span><span class='line'>183577  35
</span><span class='line'>198422  36
</span><span class='line'>212420  37
</span><span class='line'>231963  38
</span><span class='line'>260548  39
</span><span class='line'>287754  40
</span><span class='line'>309051  41
</span><span class='line'>319713  42
</span><span class='line'>324303  43
</span><span class='line'>322955  44
</span><span class='line'>314771  45
</span><span class='line'>299365  46
</span><span class='line'>286505  47
</span><span class='line'>271576  48
</span><span class='line'>250484  49
</span><span class='line'>236383  50
</span><span class='line'>223297  51
</span><span class='line'>211611  52
</span><span class='line'>202527  53
</span><span class='line'>181511  54
</span><span class='line'>165355  55
</span><span class='line'>151340  56
</span><span class='line'>144645  57
</span><span class='line'>134739  58
</span><span class='line'>124177  59
</span><span class='line'>117834  60
</span><span class='line'>108356  61
</span><span class='line'>101186  62
</span><span class='line'>91521  63
</span><span class='line'>87398  64
</span><span class='line'>84750  65
</span><span class='line'>81343  66
</span><span class='line'>77693  67
</span><span class='line'>78361  68
</span><span class='line'>77745  69
</span><span class='line'>70192  70
</span><span class='line'>66825  71
</span><span class='line'>67664  72
</span><span class='line'>64431  73
</span><span class='line'>59086  74
</span><span class='line'>50597  75
</span><span class='line'>48997  76
</span><span class='line'>49218  77
</span><span class='line'>34358  78
</span><span class='line'>35065  79
</span><span class='line'>35773  80
</span><span class='line'>38217  81
</span><span class='line'>33938  82
</span><span class='line'>31860  83
</span><span class='line'>30124  84
</span><span class='line'>27754  85
</span><span class='line'>24956  86
</span><span class='line'>23648  87
</span><span class='line'>21718  88
</span><span class='line'>19175  89
</span><span class='line'>16863  90
</span><span class='line'>15098  91
</span><span class='line'>13492  92
</span><span class='line'>11642  93
</span><span class='line'>10085  94
</span><span class='line'>8511  95
</span><span class='line'>7480  96
</span><span class='line'>5962  97
</span><span class='line'>4622  98
</span><span class='line'>3617  99
</span><span class='line'>27734  NA
</span><span class='line'>3050 100
</span><span class='line'>2666 101
</span><span class='line'>2335 102
</span><span class='line'>2003 103
</span><span class='line'>1350 104
</span><span class='line'>1280 105
</span><span class='line'> 899 106
</span><span class='line'> 594 107
</span><span class='line'> 456 108
</span><span class='line'> 265 109
</span><span class='line'> 261 110
</span><span class='line'> 252 111
</span><span class='line'> 188 112
</span><span class='line'> 117 113
</span><span class='line'>  22 114
</span><span class='line'>  82 115
</span><span class='line'>  63 116
</span><span class='line'>  14 117
</span><span class='line'>   3 126
</span><span class='line'>   8 127
</span><span class='line'>   8 163
</span><span class='line'>   3 164
</span><span class='line'>   1 "age"</span></code></pre></td></tr></table></div></figure>


<h2>8. ind_nuevo</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 8 | sort -d | uniq -c
</span><span class='line'>12808368  0
</span><span class='line'>811207  1
</span><span class='line'>27734 NA
</span><span class='line'>   1 "ind_nuevo"</span></code></pre></td></tr></table></div></figure>


<h2>10. indrel</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 10 | sort -d | uniq -c
</span><span class='line'>13594782  1
</span><span class='line'>24793 99
</span><span class='line'>27734 NA
</span><span class='line'>   1 "indrel"</span></code></pre></td></tr></table></div></figure>


<h2>12. indrel_1mes</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 12 | sort -d | uniq -c
</span><span class='line'>149781
</span><span class='line'>4357298 1
</span><span class='line'>9133383 1.0
</span><span class='line'> 577 2
</span><span class='line'> 740 2.0
</span><span class='line'>1570 3
</span><span class='line'>2780 3.0
</span><span class='line'>  83 4
</span><span class='line'> 223 4.0
</span><span class='line'> 874 P
</span><span class='line'>   1 "indrel_1mes"</span></code></pre></td></tr></table></div></figure>


<h2>13. tiprel_1mes</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 13 | sort -d | uniq -c
</span><span class='line'>149781
</span><span class='line'>6187123 A
</span><span class='line'>7304875 I
</span><span class='line'>   4 N
</span><span class='line'>4656 P
</span><span class='line'> 870 R
</span><span class='line'>   1 "tiprel_1mes"</span></code></pre></td></tr></table></div></figure>


<h2>14. indresi</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 14 | sort -d | uniq -c
</span><span class='line'>27734
</span><span class='line'>65864 N
</span><span class='line'>13553711 S
</span><span class='line'>   1 "indresi"</span></code></pre></td></tr></table></div></figure>


<h2>15. indext</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 15 | sort -d | uniq -c
</span><span class='line'>27734
</span><span class='line'>12974839 N
</span><span class='line'>644736 S
</span><span class='line'>   1 "indext"</span></code></pre></td></tr></table></div></figure>


<h2>16. conyuemp</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 16 | sort -d | uniq -c
</span><span class='line'>13645501
</span><span class='line'>1791 N
</span><span class='line'>  17 S
</span><span class='line'>   1 "conyuemp"</span></code></pre></td></tr></table></div></figure>


<h2>17. canal_entrada</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 17 | sort -d | uniq -c
</span><span class='line'>186126
</span><span class='line'> 210 004
</span><span class='line'>29063 007
</span><span class='line'>27139 013
</span><span class='line'>  11 025
</span><span class='line'> 152 K00
</span><span class='line'>66656 KAA
</span><span class='line'>62381 KAB
</span><span class='line'>7697 KAC
</span><span class='line'>10680 KAD
</span><span class='line'>50764 KAE
</span><span class='line'>30559 KAF
</span><span class='line'>74295 KAG
</span><span class='line'>24875 KAH
</span><span class='line'>37699 KAI
</span><span class='line'>24280 KAJ
</span><span class='line'> 838 KAK
</span><span class='line'>7573 KAL
</span><span class='line'>11285 KAM
</span><span class='line'>1370 KAN
</span><span class='line'>6676 KAO
</span><span class='line'>14928 KAP
</span><span class='line'>18017 KAQ
</span><span class='line'>32686 KAR
</span><span class='line'>86221 KAS
</span><span class='line'>3268209 KAT
</span><span class='line'> 249 KAU
</span><span class='line'> 107 KAV
</span><span class='line'>34275 KAW
</span><span class='line'>67350 KAY
</span><span class='line'>32186 KAZ
</span><span class='line'>1298 KBB
</span><span class='line'> 241 KBD
</span><span class='line'> 147 KBE
</span><span class='line'>3760 KBF
</span><span class='line'>1725 KBG
</span><span class='line'>7197 KBH
</span><span class='line'> 519 KBJ
</span><span class='line'> 575 KBL
</span><span class='line'> 555 KBM
</span><span class='line'>  61 KBN
</span><span class='line'>7380 KBO
</span><span class='line'>  85 KBP
</span><span class='line'>4179 KBQ
</span><span class='line'>1875 KBR
</span><span class='line'> 714 KBS
</span><span class='line'>3127 KBU
</span><span class='line'> 896 KBV
</span><span class='line'>1130 KBW
</span><span class='line'> 102 KBX
</span><span class='line'> 642 KBY
</span><span class='line'>46446 KBZ
</span><span class='line'>1530 KCA
</span><span class='line'>5187 KCB
</span><span class='line'>49308 KCC
</span><span class='line'>3276 KCD
</span><span class='line'> 309 KCE
</span><span class='line'> 420 KCF
</span><span class='line'>5399 KCG
</span><span class='line'>24098 KCH
</span><span class='line'>26546 KCI
</span><span class='line'> 215 KCJ
</span><span class='line'> 957 KCK
</span><span class='line'>4187 KCL
</span><span class='line'>3251 KCM
</span><span class='line'>1117 KCN
</span><span class='line'> 179 KCO
</span><span class='line'> 158 KCP
</span><span class='line'> 198 KCQ
</span><span class='line'> 196 KCR
</span><span class='line'> 236 KCS
</span><span class='line'> 105 KCT
</span><span class='line'>1081 KCU
</span><span class='line'> 229 KCV
</span><span class='line'>  67 KCX
</span><span class='line'> 379 KDA
</span><span class='line'>  17 KDB
</span><span class='line'>1531 KDC
</span><span class='line'> 534 KDD
</span><span class='line'> 828 KDE
</span><span class='line'> 726 KDF
</span><span class='line'> 469 KDG
</span><span class='line'> 191 KDH
</span><span class='line'>  17 KDI
</span><span class='line'>  11 KDL
</span><span class='line'>2468 KDM
</span><span class='line'> 197 KDN
</span><span class='line'>1763 KDO
</span><span class='line'>1001 KDP
</span><span class='line'>1112 KDQ
</span><span class='line'>8050 KDR
</span><span class='line'>1924 KDS
</span><span class='line'>1299 KDT
</span><span class='line'>2588 KDU
</span><span class='line'> 427 KDV
</span><span class='line'> 772 KDW
</span><span class='line'>1728 KDX
</span><span class='line'>1960 KDY
</span><span class='line'> 531 KDZ
</span><span class='line'> 851 KEA
</span><span class='line'> 370 KEB
</span><span class='line'> 231 KEC
</span><span class='line'>3011 KED
</span><span class='line'> 175 KEE
</span><span class='line'> 329 KEF
</span><span class='line'>1895 KEG
</span><span class='line'>1546 KEH
</span><span class='line'> 944 KEI
</span><span class='line'>9247 KEJ
</span><span class='line'> 542 KEK
</span><span class='line'>2483 KEL
</span><span class='line'>  68 KEM
</span><span class='line'>4917 KEN
</span><span class='line'> 850 KEO
</span><span class='line'> 146 KEQ
</span><span class='line'>5904 KES
</span><span class='line'> 265 KEU
</span><span class='line'> 844 KEV
</span><span class='line'>5687 KEW
</span><span class='line'>35146 KEY
</span><span class='line'>2433 KEZ
</span><span class='line'>409669 KFA
</span><span class='line'> 107 KFB
</span><span class='line'>3098360 KFC
</span><span class='line'>44461 KFD
</span><span class='line'> 250 KFE
</span><span class='line'>5529 KFF
</span><span class='line'>6800 KFG
</span><span class='line'>2579 KFH
</span><span class='line'> 881 KFI
</span><span class='line'>6620 KFJ
</span><span class='line'>3913 KFK
</span><span class='line'>3806 KFL
</span><span class='line'> 371 KFM
</span><span class='line'>4520 KFN
</span><span class='line'>9487 KFP
</span><span class='line'> 371 KFR
</span><span class='line'>6694 KFS
</span><span class='line'>8036 KFT
</span><span class='line'>4914 KFU
</span><span class='line'>  67 KFV
</span><span class='line'>  28 KGC
</span><span class='line'>  17 KGN
</span><span class='line'>  28 KGU
</span><span class='line'>8950 KGV
</span><span class='line'> 989 KGW
</span><span class='line'>9474 KGX
</span><span class='line'>4138 KGY
</span><span class='line'>  51 KHA
</span><span class='line'>16418 KHC
</span><span class='line'>116891 KHD
</span><span class='line'>4055270 KHE
</span><span class='line'>20674 KHF
</span><span class='line'>241084 KHK
</span><span class='line'>45128 KHL
</span><span class='line'>183924 KHM
</span><span class='line'>116608 KHN
</span><span class='line'>8992 KHO
</span><span class='line'> 691 KHP
</span><span class='line'>591039 KHQ
</span><span class='line'>   1 KHR
</span><span class='line'>   5 KHS
</span><span class='line'>75607 RED
</span><span class='line'>   1 "canal_entrada"</span></code></pre></td></tr></table></div></figure>


<h2>18. indfall</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 18 | sort -d | uniq -c
</span><span class='line'>27734
</span><span class='line'>13584813 N
</span><span class='line'>34762 S
</span><span class='line'>   1 "indfall"</span></code></pre></td></tr></table></div></figure>


<h2>22. ind_actividad_cliente</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 22 | sort -d | uniq -c
</span><span class='line'>6903158  0
</span><span class='line'>5841260  1
</span><span class='line'>429322  A"
</span><span class='line'>124933  ILLES"
</span><span class='line'>85202  LA"
</span><span class='line'>235700  LAS"
</span><span class='line'>27734 NA
</span><span class='line'>   1 "ind_actividad_cliente"</span></code></pre></td></tr></table></div></figure>


<h2>24. segmento</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat train_ver2.csv | cut -d , -f 24 | sort -d | uniq -c | head -100
</span><span class='line'>418613
</span><span class='line'>545352 01 - TOP
</span><span class='line'>7542889 02 - PARTICULARES
</span><span class='line'>4506805 03 - UNIVERSITARIO
</span><span class='line'>  17 100000.44
</span><span class='line'>  17 100001.85
</span><span class='line'>  11 100007.28
</span><span class='line'>  17 100010.67
</span><span class='line'>  17 100013.34
</span><span class='line'>  17 100013.7
</span><span class='line'>  17 100014.21</span></code></pre></td></tr></table></div></figure>


<h1>Product stats</h1>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0    13645913
</span><span class='line'>1        1396
</span><span class='line'>Name: ind_ahor_fin_ult1, dtype: int64
</span><span class='line'>0    13646993
</span><span class='line'>1         316
</span><span class='line'>Name: ind_aval_fin_ult1, dtype: int64
</span><span class='line'>1    8945588
</span><span class='line'>0    4701721
</span><span class='line'>Name: ind_cco_fin_ult1, dtype: int64
</span><span class='line'>0    13641933
</span><span class='line'>1        5376
</span><span class='line'>Name: ind_cder_fin_ult1, dtype: int64
</span><span class='line'>0    12543689
</span><span class='line'>1     1103620
</span><span class='line'>Name: ind_cno_fin_ult1, dtype: int64
</span><span class='line'>0    13518012
</span><span class='line'>1      129297
</span><span class='line'>Name: ind_ctju_fin_ult1, dtype: int64
</span><span class='line'>0    13514567
</span><span class='line'>1      132742
</span><span class='line'>Name: ind_ctma_fin_ult1, dtype: int64
</span><span class='line'>0    11886693
</span><span class='line'>1     1760616
</span><span class='line'>Name: ind_ctop_fin_ult1, dtype: int64
</span><span class='line'>0    13056301
</span><span class='line'>1      591008
</span><span class='line'>Name: ind_ctpp_fin_ult1, dtype: int64
</span><span class='line'>0    13623034
</span><span class='line'>1       24275
</span><span class='line'>Name: ind_deco_fin_ult1, dtype: int64
</span><span class='line'>0    13624641
</span><span class='line'>1       22668
</span><span class='line'>Name: ind_deme_fin_ult1, dtype: int64
</span><span class='line'>0    13060928
</span><span class='line'>1      586381
</span><span class='line'>Name: ind_dela_fin_ult1, dtype: int64
</span><span class='line'>0    12518082
</span><span class='line'>1     1129227
</span><span class='line'>Name: ind_ecue_fin_ult1, dtype: int64
</span><span class='line'>0    13395025
</span><span class='line'>1      252284
</span><span class='line'>Name: ind_fond_fin_ult1, dtype: int64
</span><span class='line'>0    13566973
</span><span class='line'>1       80336
</span><span class='line'>Name: ind_hip_fin_ult1, dtype: int64
</span><span class='line'>0    13522150
</span><span class='line'>1      125159
</span><span class='line'>Name: ind_plan_fin_ult1, dtype: int64
</span><span class='line'>0    13611452
</span><span class='line'>1       35857
</span><span class='line'>Name: ind_pres_fin_ult1, dtype: int64
</span><span class='line'>0    12930329
</span><span class='line'>1      716980
</span><span class='line'>Name: ind_reca_fin_ult1, dtype: int64
</span><span class='line'>0    13041523
</span><span class='line'>1      605786
</span><span class='line'>Name: ind_tjcr_fin_ult1, dtype: int64
</span><span class='line'>0    13297834
</span><span class='line'>1      349475
</span><span class='line'>Name: ind_valo_fin_ult1, dtype: int64
</span><span class='line'>0    13594798
</span><span class='line'>1       52511
</span><span class='line'>Name: ind_viv_fin_ult1, dtype: int64
</span><span class='line'>0.0    12885285
</span><span class='line'>1.0      745961
</span><span class='line'>Name: ind_nomina_ult1, dtype: int64
</span><span class='line'>0.0    12821161
</span><span class='line'>1.0      810085
</span><span class='line'>Name: ind_nom_pens_ult1, dtype: int64
</span><span class='line'>0    11901597
</span><span class='line'>1     1745712
</span><span class='line'>Name: ind_recibo_ult1, dtype: int64</span></code></pre></td></tr></table></div></figure>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/15/ghost-blog-as-your-github-profile-page/">Ghost Blog as Your Github User Page</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-09-15T19:54:00+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>15</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>7:54 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Ghost is a minimalistic blogging platform that is very easy to setup. You can use it as your github user page using a tool called <a href="https://github.com/axitkhurana/buster">buster</a></p>

<p>My current github pages are setup using Octopress, both ghost and octopress has its advantages and disadvantages. Every time I&rsquo;m making a new post using octopress, I have to go to some online markdown editor to write my post. Where as Ghost has its inbuilt markdown editor where you can preview your changes as you type. That can be a little annoying. Even though my current setup still uses octopress, I am editing this markdown using Ghost. One disadvantage of ghost is that it stores all the my blog posts in a local sqlite db, where as in octopress, I can see my markdown files and edit and commit changes to the main repository.</p>

<h2>3 Steps</h2>

<ul>
<li>Setup Ghost Blog</li>
<li>Generate Static Content using <a href="https://github.com/axitkhurana/buster">Buster</a></li>
<li>Gotchas.</li>
</ul>


<h3>Setting up Ghost</h3>

<ul>
<li>Download ghost from <a href="https://ghost.org/download/">here.</a></li>
<li>Install npm dependencies.</li>
<li>npm start to run it on your local machine port 2368.</li>
<li>Click <a href="http://localhost:2368/ghost">here</a> to register a new user and start posting new blog posts.</li>
<li>If you want, you can download additional themes into /content/themes folder and change the theme in your <a href="http://localhost:2368/ghost/settings">ghost settings.</a></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; npm install
</span><span class='line'>&gt; npm start</span></code></pre></td></tr></table></div></figure>


<h3>Generating Static Pages from your ghost blog using <a href="https://github.com/axitkhurana/buster">Buster.</a></h3>

<ul>
<li>You need wget for buster python package to work.</li>
<li>Download and install wget and its dependencies from <a href="http://gnuwin32.sourceforge.net/packages/wget.htm">here.</a></li>
<li>Install <em>libintl-2</em>, <em>libiconv-2</em>, <em>openssl</em> along with <em>wget</em> as given in the end of the wget download page.</li>
<li>You can install <strong>buster</strong> using pip, and can use <em>buster</em> directly from the command line, but you are likely to face problems if you are using Windows. Instead you can just clone the buster repo and use the <strong>buster.py</strong> directly.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; pip install buster
</span><span class='line'>&gt; buster setup --gh-repo=&lt;repo-url&gt;
</span><span class='line'>&gt; buster generate --domain=http://localhost:2368</span></code></pre></td></tr></table></div></figure>


<h4>or</h4>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; git clone http://github.com/axitkhurana/buster
</span><span class='line'>&gt; cd buster
</span><span class='line'>&gt; python buster/buster.py setup --gh-repo=&lt;repo-url&gt;
</span><span class='line'>&gt; python buster/buster.py generate --domain=http://localhost:2386</span></code></pre></td></tr></table></div></figure>


<ul>
<li><strong><em>setup</em></strong> argumet will ask you for the github repo of your userpage and creates a directory named <strong>static</strong></li>
<li><strong><em>generate</em></strong> argument will recursively download all the static files of a given url into <strong>static</strong> folder.</li>
<li><strong><em>preview</em></strong> argument will start a simple python webserver and hosts the static files in your <strong>static</strong> folder on your localmachine port 9000, so that you can preview the static files genereated by your previous command.</li>
<li><strong><em>deploy</em></strong> will push changes to your github repository</li>
</ul>


<h3>Gotchas</h3>

<ul>
<li><strong>wget</strong> has some problems downloading a webpage using <strong>generate</strong> command because of \\ slashes so remove those according to this <a href="https://github.com/syllogismos/buster/commit/ba2c8df24b78361699e767891f14ed63d775ec21#diff-a61cc6042865036a60870334dd92047cL39">commit.</a></li>
<li>When previewing the static site generated, <strong>.css</strong> and <strong>.js</strong> using pythons SimpleHttpServer, the MIME type is changed to <strong>application/octet-stream</strong>. In order to fix this we have an additional class named <strong>ExtHandler</strong> that extends <em>SimpleHttpRequestHandler</em> to change the default <em>get_type()</em> behaviour.</li>
<li>This <a href="https://github.com/syllogismos/buster/commit/ba2c8df24b78361699e767891f14ed63d775ec21#diff-a61cc6042865036a60870334dd92047cL39">commit</a> has both of the above changes.</li>
<li>If you are on windows, I recommend cloning <a href="http://github.com/syllogismos/buster">this fork</a> of buster instead of the original, that has both of the above fixes.</li>
<li>Some static files aren&rsquo;t downloaded using the <strong>generate</strong> command, my understanding is that the static files that are requested from javascript or fonts requested using css aren&rsquo;t generated. You are better of copying the static files and images that aren&rsquo;t in your static folder directly.</li>
</ul>


<h3>Summary</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>## download ghost
</span><span class='line'>&gt; cd ghost
</span><span class='line'>&gt; npm install
</span><span class='line'>&gt; npm start
</span><span class='line'>## ghost blog is in http://localhost:2368
</span><span class='line'>## admin/settings in http://localhost:2368/ghost
</span><span class='line'>
</span><span class='line'>## change to another directory where you want your generated static website to reside
</span><span class='line'>&gt; git clone https://github.com/syllogismos/buster
</span><span class='line'>&gt; cd buster
</span><span class='line'>
</span><span class='line'>&gt; python buster/buster.py setup
</span><span class='line'>## give your github page repository, it creates a static dir
</span><span class='line'>
</span><span class='line'>&gt; python buster/buster.py generate --domain=http://localhost:2368
</span><span class='line'>## downloads all the static files into your static dir
</span><span class='line'>
</span><span class='line'>&gt; python buster/buster.py preview
</span><span class='line'>## to see your downloaded static files are proper
</span><span class='line'>## If you see any of the static files missing, say fonts or images, just copy them from your ghost directory
</span><span class='line'>## make sure your generated static files look okay
</span><span class='line'>
</span><span class='line'>&gt; python buster/buster.py deploy # to commit your changes to the github repo</span></code></pre></td></tr></table></div></figure>


<p>Thank you.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/13/stochastic-gradient-descent/">Stochastic Gradient Descent in AD.</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-09-13T19:17:43+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>13</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>7:17 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In <strong>stochastic gradient descent</strong>, the true gradient is approximated by gradient at each single example.</p>

<p><img src="http://upload.wikimedia.org/math/7/d/9/7d9f6671a202d94d26730ef898d8d4f2.png" alt="update rule" /></p>

<p>As the algorithm sweeps through the training set, it performs the above update for each training example. Several passes can be made over the training set until the algorithm converges, if this is done, the data can be shuffled for each pass to prevent cycles.</p>

<p>Obviously it is faster than normal gradient descent, cause we don&rsquo;t have to compute  cost function over the entire data set in each iteration in case of stochastic gradinet descent.</p>

<h2>stochasticGradientDescent in AD:</h2>

<p>This is my implementation of Stochastic Gradient Descent in AD library, you can get it from <a href="http://github.com/syllogismos/ad">my fork</a> of AD.</p>

<p>Its type signature is</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>stochasticGradientDescent :: (Traversable f, Fractional a, Ord a) 
</span><span class='line'>  =&gt; (forall s. Reifies s Tape =&gt; f (Scalar a) -&gt; f (Reverse s a) -&gt; Reverse s a) 
</span><span class='line'>  -&gt; [f (Scalar a)]
</span><span class='line'>  -&gt; f a 
</span><span class='line'>  -&gt; [f a]</span></code></pre></td></tr></table></div></figure>


<p></p>

<h4>Its arguments are:</h4>

<ul>
<li><code>errorSingle :: (forall s. Reifies s Tape =&gt; f (Scalar a) -&gt; f (Reverse s a) -&gt; Reverse s a)</code> function, that computes error in a single training sample given <code>theta</code></li>
<li>Entire training data, you should be able to map the above <code>errorSingle</code> function over the training data.</li>
<li>and initial Theta</li>
</ul>


<h2>Example:</h2>

<p><a href="https://raw.githubusercontent.com/syllogismos/machine-learning-haskell/master/exampledata.txt">Here</a> is the sample data I&rsquo;m running <code>stochasticGradientDescent</code> on.</p>

<p>Its just 97 rows of samples with two columns, first column is <code>y</code> and the other is <code>x</code></p>

<p>Below is our error function, a simple squared loss error function. You can introduce regularization here if you want.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>errorSingle :: 
</span><span class='line'>  forall a. (Floating a, Mode a) 
</span><span class='line'>  =&gt; [Scalar a] 
</span><span class='line'>  -&gt; [a] 
</span><span class='line'>  -&gt; a
</span><span class='line'>errorSingle d0 theta = sqhalf $ costSingle (tail d0) theta - auto ( head d0)
</span><span class='line'>  where
</span><span class='line'>    sqhalf t = (t**2)/2
</span><span class='line'>    
</span><span class='line'>costSingle x' theta' = constant + sum (zipWith (*) coeff autox')
</span><span class='line'>      where
</span><span class='line'>        constant = head theta'
</span><span class='line'>        autox' = map auto x'
</span><span class='line'>        coeff = tail theta'</span></code></pre></td></tr></table></div></figure>


<p>Running Stochastic Gradient Descent:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>lambda: a &lt;- readFile "exampledata.txt"
</span><span class='line'>lambda: let d = lines a
</span><span class='line'>lambda: let train = map ((read :: String -&gt; [Float]) . (\tmp -&gt; "[" ++ tmp ++ "]")) d
</span><span class='line'>lambda: let sgdRegressor = stochasticGradientDescent errorSingle train
</span><span class='line'>
</span><span class='line'>lambda: sgdRegressor [0, 0] !! 96
</span><span class='line'>[0.2981517,1.2027082]
</span><span class='line'>(0.03 secs, 4228764 bytes)
</span><span class='line'>
</span><span class='line'>lambda: sgdRegressor [0, 0] !! (97*2 -1)
</span><span class='line'>[0.49144596,1.1814859]
</span><span class='line'>(0.03 secs, 2097796 bytes)
</span><span class='line'>
</span><span class='line'>lambda: sgdRegressor [0, 0] !! (97*3 -1)
</span><span class='line'>[0.67614514,1.1605322]
</span><span class='line'>(0.03 secs, 2647504 bytes)
</span><span class='line'>
</span><span class='line'>lambda: sgdRegressor [0, 0] !! (97*4 -1)
</span><span class='line'>[0.8526818,1.1405041]
</span><span class='line'>(0.03 secs, 3158452 bytes)
</span><span class='line'>
</span><span class='line'>lambda: sgdRegressor [0, 0] !! (97*5 -1)
</span><span class='line'>[1.0214167,1.1213613]
</span><span class='line'>(0.05 secs, 3707068 bytes)</span></code></pre></td></tr></table></div></figure>


<h2>Cross checking with <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html">SGDRegressor</a> from scikit-learn</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; import csv
</span><span class='line'>&gt; import numpy as np
</span><span class='line'>&gt; from sklearn import linear_model
</span><span class='line'>
</span><span class='line'>&gt; f = open('exampledata.txt', 'r')
</span><span class='line'>&gt; fcsv = csv.reader(f)
</span><span class='line'>
</span><span class='line'>&gt; d = []
</span><span class='line'>&gt; try:
</span><span class='line'>&gt;    while True:
</span><span class='line'>&gt;        d.append(fcsv.next())
</span><span class='line'>&gt; except:
</span><span class='line'>&gt;     pass
</span><span class='line'>&gt; f.close()
</span><span class='line'>
</span><span class='line'>&gt; for i in range(len(d)):
</span><span class='line'>&gt;     for j in range(2):
</span><span class='line'>&gt;         d[i][j] = float(d[i][j])
</span><span class='line'>
</span><span class='line'>&gt; x = []
</span><span class='line'>&gt; y = []
</span><span class='line'>&gt; for i in range(len(d)):
</span><span class='line'>&gt;     x.append(d[i][1:])
</span><span class='line'>&gt;     y.append(d[i][0])
</span><span class='line'>
</span><span class='line'># initial learning rate eta0 = 0.001
</span><span class='line'># learning rate is constant
</span><span class='line'># regularization parameter alpha = 0.0, as we ignored reqularization
</span><span class='line'># loss function = squared_loss
</span><span class='line'># n_iter or epoch, how many times does the algorithm pass our training data.
</span><span class='line'>&gt; reg = linear_model.SGDRegressor(alpha=0.0, eta0=0.001, loss='squared_loss',n_iter=1, learning_rate='constant' )
</span><span class='line'># start training with initial theta of 0, 0
</span><span class='line'>&gt; sgd = reg.fit(x,y, coef_init=[0], intercept_init=[0])
</span><span class='line'>&gt; print [sgd.intercept_, sgd.coef_]
</span><span class='line'>[array([ 0.29815173]), array([ 1.20270826])]</span></code></pre></td></tr></table></div></figure>


<p>The only restriction we have in our implementation of stochasticGradientDescent is that we set the learning rate a default value of 0.001 and is a constant through out the algorithm.</p>

<p>The rest of the things like the sort of regulariztion, regularization parameter, loss function we are using, we can specify in <code>errorSingle</code>.</p>

<h2>Results:</h2>

<p>So when <code>n_iter = 1</code>, went through the entire data set once, so we must check <code>97th</code> theta from our regression result from <strong>AD</strong>.
Similarly <code>n_iter = 2</code> implies <code>97*2</code> iteration in our implementation, and etc.,</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>n-iter = 1, i = 96
</span><span class='line'>scikit-learn: [array([ 0.29815173]), array([ 1.20270826])]
</span><span class='line'>AD: [0.2981517,1.2027082]
</span><span class='line'>
</span><span class='line'>n-iter = 2, i = 97x2 - 1
</span><span class='line'>scikit-learn: [array([ 0.49144583]), array([ 1.18148583])]  
</span><span class='line'>AD: [0.49144596,1.1814859]
</span><span class='line'>
</span><span class='line'>n-iter = 3, i = 97x3 - 1  
</span><span class='line'>scikit-learn: [array([ 0.67614512]), array([ 1.16053217])]  
</span><span class='line'>AD: [0.67614514,1.1605322]
</span><span class='line'>
</span><span class='line'>n-iter = 4, i = 97x4 - 1  
</span><span class='line'>scikit-learn: [array([ 0.85268182]), array([ 1.14050415])]  
</span><span class='line'>AD: [0.8526818,1.1405041]
</span><span class='line'>
</span><span class='line'>n-iter = 5, i = 97X5 -1  
</span><span class='line'>scikit-learn: [array([ 1.02141669]), array([ 1.12136124])]  
</span><span class='line'>AD: [1.0214167,1.1213613]</span></code></pre></td></tr></table></div></figure>


<p><a href="http://www.github.com/syllogismos/machine-learning-haskell">Here</a> in this repository, you can find the ipython notebook and haskell code so that you can test these yourself.</p>

<h2>References:</h2>

<ol>
<li><a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent on wikipedia</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html">SGDRegressor from scikit-learn</a></li>
<li><a href="http://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent">Gradient Descent vs Stochastic Gradient Descent</a></li>
<li><a href="http://metaoptimize.com/qa/questions/10046/batch-gradient-descent-vs-stochastic-gradient-descent">Batch Gradient Descent vs Stochastic Gradient Descent</a></li>
<li><a href="http://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent">Batch Gradient Descent vs Stochastic Gradient Descent</a></li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/04/tinder-like-android-app-that-showcases-products-from-a-fashion-website/">Tinder Like Android App That Showcases Products From a Fashion Website</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-09-04T08:07:44+05:30'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>4</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>8:07 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><ul>
<li>Github repo: <a href="https://github.com/syllogismos/MyntraTinder">https://github.com/syllogismos/MyntraTinder</a></li>
<li>Android app that implements tinder ui, based on <a href="https://github.com/exctasy2/tinder-card-stack">https://github.com/exctasy2/tinder-card-stack</a></li>
<li>Latest apk build file <a href="https://github.com/syllogismos/MyntraTinder/blob/master/app-debug.apk">here</a></li>
<li>Current build is pretty decent, I&rsquo;m pretty happy with it logically, UI can be improved.</li>
</ul>


<h2>UI</h2>

<p><img src="http://i.imgur.com/d7o9Ccz.png" title="Navigation Drawer" alt="Navigation Drawer" />
<img src="http://i.imgur.com/FbgLOf4.png" title="Like" alt="Swipe Right" />
<img src="http://i.imgur.com/sTMeDDr.png" title="Dislike" alt="Swipe Left" /></p>

<h2>Relavent Activities, Fragments, Views, Models, Resources, Layouts and Adapters</h2>

<h3>Activities:</h3>

<ul>
<li><p><strong>MyntraTinderActivity</strong>: This is the main activity that is based on the default DrawerLayoutActivity provided
to us in Android Studio.<br/>
It extends <strong>NavigationDrawerCallbacks</strong> from <strong>NavigationDrawerFragmentSingleElv</strong> to handle click events
in the drawer fragment.</p>

<ul>
<li><em>Layout</em>: <em>activity_myntra_tinder.xml</em></li>
</ul>
</li>
</ul>


<p>The rest of the activities in the project are just for testing and trying various things.</p>

<h3>Fragments:</h3>

<ul>
<li><p><strong>NavigationDrawerFragmentSingleElv</strong>: Fragment that implements the navigation drawer, that has a single
<em>ExpandbleListView</em> where you can find all the product groups, you click on the relavent product group
to get products presented to you in the form a Tinder Stack. It also has a list view for things like
&ldquo;Home&rdquo;, &ldquo;Settings&rdquo; etc.,</p>

<ul>
<li><em>Layouts</em>: <em>fragment_navigation_drawer_myntra_tinder_activity_single_elv.xml</em></li>
<li><em>Adapters</em>: <strong>MyntraCategoryExpandableListAdapter</strong>, to populate various product categories to our <em>ExpandableListView</em></li>
</ul>
</li>
<li><p><strong>TinderUIFragment</strong>: Fragment that has the ProductStackView with in which we showcase a given list of products
in a tinder stack.</p>

<ul>
<li><em>Layouts</em>: <em>fragment_tinder_ui.xml</em></li>
<li><em>Adapters</em>:</li>
</ul>
</li>
<li><p><strong>HomeFragment</strong>: Place holder fragment that you can further develop upon to show what ever you want, currently,
this fragment shows up when you click ListView items like &ldquo;Home&rdquo;, &ldquo;Settings&rdquo;.</p>

<ul>
<li><em>Layouts</em>: <em>fragment_home.xml</em></li>
<li><em>Adapters</em>:</li>
</ul>
</li>
<li><p><strong>LikedProductsFragment</strong>: Fragment that contains a single ListView that is used to display all the liked products from
a single product group. This is implemented inside the <strong>MyntraTinderActivity</strong></p>

<ul>
<li><em>Layouts</em>: <em>activity_product_list_view.xml</em></li>
<li><em>Adapters</em>: <strong>ProductListAdapterWithACursor</strong>, a cursor adapter that takes a cursor obtained from a database
operation and fills our listView</li>
</ul>
</li>
</ul>


<h3>Views:</h3>

<ul>
<li><p><strong>SingleProductView</strong>: View that implements what each Card in the TinderStack looks like, this is what you swipe right
or left to like or dislike a particular product.</p>

<ul>
<li><em>Layouts</em>: <em>product_card.xml</em></li>
<li><em>Adapters</em>:</li>
</ul>
</li>
<li><p><strong>ProductStackView</strong>: View that contains the whole stack,</p>

<ul>
<li><em>Layouts</em>:</li>
<li><em>Adapters</em>: <strong>ProductCardAdapter</strong>, adapter that loads, given list of products in to our tinder stack, you can find
helper functions inside this adapter and see how we initialize the adapter in various cases.</li>
</ul>
</li>
</ul>


<h3>Models:</h3>

<ul>
<li><strong>Product</strong>: A model that defines a single Product and its parameters</li>
<li><strong>MyntraCategory</strong>: A model that defines product categories, sub categories etc., and also an helper function to load
these into our ExpandableListView.</li>
</ul>


<h3>Utils:</h3>

<ul>
<li><strong>DatabaseHelper</strong>: Helper functions that handle all database operations and our db schema is also defined in this.</li>
<li><strong>Downloader</strong>: Helper functions to download json, to our phones filesystem, given url, postData and filename</li>
<li><strong>ProductsJSONPullParser</strong>: Helper functions to parse a given json file and return a list of Products.</li>
</ul>


<h2>Additional Notes:</h2>

<h4>Note1:</h4>

<p>When you select a product category/group in the expandable list view, it queries the database for 20 products from that
category, if the db doesn&rsquo;t even have a single product from that category, it gets new products from the server side, 96
at a time and fill the db with new products. And returns 20 new products to the ProductStackAdapter</p>

<h4>Note2:</h4>

<p>Our product categories have 3 levels of hierarchy, but ExpandableListView has only 2 levels, so the top most level,
say &ldquo;Men&rdquo; is also technically a &ldquo;Group&rdquo; with no children, it only behaves as a header to the next two levels in the hierarchy.
Below is the actual hierarchy of product groups.</p>

<ul>
<li>Men

<ul>
<li>Footwear

<ol>
<li>Casual Shoes</li>
<li>Formal Shoes</li>
</ol>
</li>
<li>Accessories

<ol>
<li>Watches</li>
<li>Sunglasses</li>
</ol>
</li>
</ul>
</li>
<li>Women

<ul>
<li>Footwear

<ol>
<li>Heels</li>
<li>Wedges</li>
</ol>
</li>
<li>Accessories

<ol>
<li>Watches</li>
<li>Sunglasses</li>
</ol>
</li>
</ul>
</li>
</ul>


<p>But for our convenience, to put it inside an ExpandableListView, we changed it to below</p>

<ul>
<li>Men</li>
<li>Footwear

<ol>
<li>Casual Shoes</li>
<li>Formal Shoes</li>
</ol>
</li>
<li>Accessories

<ol>
<li>Watches</li>
<li>Sunglasses</li>
</ol>
</li>
<li>Women</li>
<li>Footwear

<ol>
<li>Heels</li>
<li>Wedges</li>
</ol>
</li>
<li>Accessories

<ol>
<li>Watches</li>
<li>Sunglasses</li>
</ol>
</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/26/a-boilerplate-nodejs-twitter-bot-that-responds-to-twitter-mentions/">A Boilerplate Nodejs Twitter Bot That Responds to Twitter Mentions.</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-08-26T23:20:56+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>26</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>11:20 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Start by forking this <a href="https://github.com/syllogismos/twitter-bot-template">github repo</a></p>

<h2>Config:</h2>

<p>Make a copy of config.template.json named config.json, and fill your secret keys of your twitter bot that you obtain from <a href="https://apps.twitter.com">here</a> and make sure your twitter app has both read and write access in the &ldquo;permissions&rdquo; tab.</p>

<h2>Installation</h2>

<p>Install all the node dependencies.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; npm install</span></code></pre></td></tr></table></div></figure>


<h2>Your bot code.</h2>

<p>The bot is written in coffeescript, and the compiled javascript is also provided in case if you prefer that.</p>

<p>At the least you need to fill the function <strong>solve</strong> whose only argument is the tweet text, include all the mentions. Not the Tweet Object, its just the tweet text.</p>

<p>You are also provided with a function <strong>isOfWrongFormat</strong> that defaults to <em>false</em> which checks the validity of the tweet text. And its only argument is the tweet text as well. Not the tweet object.</p>

<h2>Running the bot.</h2>

<p>If you wrote the bot in coffee-script do this</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; coffee twitterBot.coffee</span></code></pre></td></tr></table></div></figure>


<p>Or if you wrote it in plain javascript do this</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; node twitterBot.js</span></code></pre></td></tr></table></div></figure>


<p>If you want to compile your coffee-script to plain javascript you can run the coffee command with a <em>-c</em> flag</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; coffee -c twitterBot.coffee</span></code></pre></td></tr></table></div></figure>


<h2>Sample Bot</h2>

<p>You can find a sample twitter bot that I wrote based on this, albeit slightly modified is <a href="https://github.com/syllogismos/countdownbot">countdownbot</a></p>

<p>It responds with a solution of the <strong>numbers game</strong> from the game show <strong>Countdown</strong>, if someone mentions the bot along with the target number and the rest of the numbers..</p>

<p>After you fill the config in the countdownbot as shown above, and ran it, Anyone mentioning your bot along with a set of numbers, with the first one being the target number</p>

<p>@someRandomPerson: @countdownbot 347 2 3 10 75 23 12</p>

<p>it responds like this.</p>

<p>@countdownbot: @someRandomPerson One possible solution for 347 is: (10*(12+23))-3</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/10/octopress-blog-as-user-page-in-github/">Octopress Blog as User Page in Github, Using Windows</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-08-10T16:15:42+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>4:15 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Step by step instructions to install Octopress blog on Windows to setup your github use page.</p>

<ul>
<li>Download RubyInstaller and ruby dev kit from <a href="http://dl.bintray.com/oneclick/rubyinstaller/rubyinstaller-1.9.3-p545.exe?direct">here</a> and <a href="https://github.com/downloads/oneclick/rubyinstaller/DevKit-tdm-32-4.5.2-20111229-1559-sfx.exe">here</a></li>
<li>The above installer installs Ruby 1.9.3, eventhough the most recent stable version > 2.0.0</li>
<li>Go to the directory where the dev-kit is installed and do the following</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; cd ruby-dev-kit
</span><span class='line'>&gt; ruby dk.rb init
</span><span class='line'>&gt; ruby dk.rb install</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Setup Octopress, change to the directory where you want your blog to reside</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; git clone git://github.com/imathis/octopress.git octopress
</span><span class='line'>&gt; cd octopress</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Install dependencies</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; gem install bundler
</span><span class='line'>&gt; bundle install</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Install default Octopress theme.</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; rake install</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Configure your blog by updating _config.yml, name of your blog, your name and things like that.</li>
<li>Create a new repo of the form YOUR-GITHUB-USER-NAME.github.io in github</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; rake setup_github_pages
</span><span class='line'># this command will ask your for your github pages url, so type 
</span><span class='line'>https://github.com/YOUR-GITHUB-USER-NAME/YOUR-GITHUB-USER-NAME.github.io</span></code></pre></td></tr></table></div></figure>


<ul>
<li>and run the following commands to deploy your local blog to github</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; rake generate
</span><span class='line'>&gt; rake deploy
</span><span class='line'># what this does is basically makes the master branch of your github repo contain all the generated
</span><span class='line'># files namely _deploy folder in your directory.</span></code></pre></td></tr></table></div></figure>


<ul>
<li>If everything worked fine, you will be able to see your blog with the defalt octopress template on YOUR-GITHUB-USER-NAME.github.io</li>
<li>Every time you update your blog you need ro do <em>rake generate</em> and <em>rake deploy</em> these commands will push your changes to your master branch on the remote</li>
<li>You can make a new post using <em>rake new_post</em> command</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; rake new_post["My first Blog Post"]</span></code></pre></td></tr></table></div></figure>


<ul>
<li>The above command creates a new markdown file in source/_posts folder, write your blog in markdown</li>
<li>Commit the changes you made locally in your local branch <em>source</em></li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; git status
</span><span class='line'># this will show that there are changes in the folder source/_posts
</span><span class='line'>&gt; git add .
</span><span class='line'>&gt; git commit -m "my first blog post"</span></code></pre></td></tr></table></div></figure>


<ul>
<li>If you want can create a new branch called <em>source</em> in your remote repository for the source files</li>
</ul>


<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; git push origin source</span></code></pre></td></tr></table></div></figure>


<h2>This is what you do everytime you create a new post</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&gt; rake new_post["new blog post"]
</span><span class='line'>&gt; rake generate
</span><span class='line'>&gt; rake deploy # to deploy static files in the remote master branch.
</span><span class='line'>&gt; git add . # or you can specify the markdown file
</span><span class='line'>&gt; git commit -m "new blog post"
</span><span class='line'>&gt; git push origin source # to push the source files to the source branch</span></code></pre></td></tr></table></div></figure>


<p>I only did this because my newly minted blog doesn&rsquo;t contain many entries :D</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/10/facebook-link-prediction/">Facebook Link Prediction</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-08-10T08:11:37+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>8:11 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>note: I did this just as an exercise, you get much more from <a href="http://blog.echen.me/2012/07/31/edge-prediction-in-a-social-graph-my-solution-to-facebooks-user-recommendation-contest-on-kaggle/">this post</a>.</p>

<h2>Link Prediction:</h2>

<p>We are given snapshot of a network and would like to infer which which interactions among existing
members are likely to occur in the near future or which existing interactions are we missing. The challenge is to effectively combine the information from the network structure with rich node and edge
attribute data.</p>

<h2>Supervised Random Walks:</h2>

<p>This repository is the implementation of Link prediction based on the paper Supervised Random Walks by  Lars Backstrom et al. The essence of which is that we combine the information from the network structure with the node and edge level attributes using Supervised Random Walks. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function.</p>

<h2>Problem Description:</h2>

<p>We are given a directed graph <em>G(V,E)</em>, a node <em>s</em> and a set of candidates to which <em>s</em> could create an edge. We label nodes to which <em>s</em> creates edges in the future as <em>destination nodes D = {d<sub>1</sub>,..,d<sub>k</sub>}</em>, while we call the other nodes to which s does not create edges no-link nodes <em>L = {l<sub>1</sub>,..,l<sub>n</sub>}</em>. We label candidate nodes with a set <em>C = D union L</em>. <em>D</em> are positive training examples and <em>L</em> are negative training examples. We can generalize this to multiple instances of <em>s, D, L</em>. Each node and each edge in G is further described with a set of features. We assume that each edge <em>(u,v)</em> has a corresponding feature vector psi<sub>uv</sub> that describes u and v and the interaction attributes.</p>

<p>For each edge (u,v) in G we compute the strength <em>a<sub>uv</sub> = f<sub>w</sub>(psi<sub>uv</sub>)</em>. Function <em>f<sub>w</sub></em> parameterized by <em>w</em> takes the edge feature vector <em>psi<sub>uv</sub></em> as input and computes the corresponding edge strength <em>a<sub>uv</sub></em> that models the random walk transition probability. It is exactly the function <em>f<sub>w</sub>(psi)</em> we learn in the training phase of the algorithm.</p>

<p>To predict new edges to <em>s</em>, first edge strengths of all edges are calculated using <em>f<sub>w</sub></em>. Then random walk with restarts is run from <em>s</em>. The stationary distribution <em>p</em> of random walk assigns each node <em>u</em> a probability <em>p<sub>u</sub></em>. Nodes are ordered by <em>p<sub>u</sub></em> and top ranked nodes are predicted as future destination nodes to <em>s</em>. The task is to learn the parameters <em>w</em> of function <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> that assigns each edge a transition probability. One can think of the weights <em>a<sub>uv</sub></em> as edge strengths and the random walk is more likely to traverse edges of high strength and thus nodes connected to node <em>s</em> via paths of strong edges will likely to be visited by the random walk and will thus rank higher.</p>

<h3>The optimization problem:</h3>

<p>The training data contains information that source node <em>s</em> will create edges to node <em>d subset D</em> and not <em>l subset L</em>. So we set parameters <em>w</em> of the function <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> so that it will assign edge weights <em>a<sub>uv</sub></em> in such a way that the random walk will be more likely to visit nodes in <em>D</em> than <em>L</em>, <em>i.e.,</em> <em>p<sub>l</sub> &lt; p<sub>d</sub></em> for each <em>d subset D</em> and <em>l subset L</em>. And thus we define the optimization problem as follows.<br/>
<img src="http://i.imgur.com/zMjJ1Nb.png" alt="optimization problem hard version" /></p>

<p>where <em>p</em> is the vector of pagerank scores. Pagerank scores <em>p<sub>i</sub></em> depend on edge strength on <em>a<sub>uv</sub></em> and thus actually depend on <em>f<sub>w</sub>(psi<sub>uv</sub>)</em> which is parameterized by <em>w</em>. The above equation (1) simply states that we want to find <em>w</em> such that the pagerank score of nodes in <em>D</em> will be greater than the scores of nodes in <em>L</em>. We prefer the shortest <em>w</em> parameters simply for the sake of regularization. But the above equation is the &ldquo;hard&rdquo; version of the optimization problem. However it is unlikely that a solution satisfying all the constraints exist. We make the optimization problem &ldquo;soft&rdquo; by introducing a loss function that penalizes the violated constraints. Now the optimization problem becomes,<br/>
<img src="http://i.imgur.com/oZ2pYN1.png" alt="optimization problem soft version." /><br/>
where lambda is the regularization parameter that trades off between the complexity(norm of <em>w</em>) for the fit of the model(how much the constraints can be violated). And <em>h(.)</em> is a loss function that assigns a non-negative penalty according to the difference of the scores <em>p<sub>l</sub>-p<sub>d</sub></em>. <em>h(.) = 0</em> if <em>p<sub>l</sub> &lt; p<sub>d</sub></em> as the constraint is not violated and <em>h(.) > 0</em> if <em>p<sub>l</sub> > p<sub>d</sub></em></p>

<h3>Solving the optimization problem:</h3>

<p>First we need to establish connection between the parameters <em>w</em> and the random walk scores <em>p</em>. Then we show how to obtain partial derivatives of the loss function and <em>p</em> with respect to <em>w</em> and then perform gradient descent to obtain optimal values of <em>w</em> and minimize loss.
We build a random walk stochastic transition matrix <em>Q<sup>&lsquo;</sup></em> from the edge strengths <em>a<sub>uv</sub></em> calculated from <em>f<sub>w</sub>(psi<sub>uv</sub>)</em>.<br/>
<img src="http://i.imgur.com/JiSHf7t.png" alt="Q dash" /></p>

<p>To obtain the final random walk transition probability matrix <em>Q</em>, we also incorporate the restart probability <em>alpha</em>, <em>i.e.,</em> the probability with which the random walk jumps back to seed node <em>s</em>, and thus &ldquo;restarts&rdquo;.<br/>
<img src="http://i.imgur.com/vE2P7LJ.png" alt="Q" /></p>

<p>each entry <em>Q<sub>uv</sub></em> deﬁnes the conditional probability that a walk will traverse edge (u, v) given that it is currently at node u.
The vector <em>p</em> is the stationary distribution of the Random Walk with restarts(also known as Personalized Page Rank), and is the solution to the following eigen vector equation.<br/>
<img src="http://i.imgur.com/UFwnobA.png" alt="eigen vector equation" /></p>

<p>The above equation establishes the connection between page rank scores <em>p</em> and the parameters <em>w</em> via the random walk transition probability matrix <em>Q</em>. Our goal now is to minimize the soft version of the loss function(eq. 2) with respect to parameter vector <em>w</em>. We do this by obtaining the gradient of <em>F(w)</em> with respect to <em>w</em>, and then performing gradient based optimization method to find <em>w</em> that minimize <em>F(w)</em>. This gets complicated due to the fact that equation 4 is recursive. For this we introduce <em>delta<sub>ld</sub> = p<sub>l</sub>-p<sub>d</sub></em> and then we can write the derivative<br/>
<img src="http://i.imgur.com/FhZVZEB.png" alt="delta ld" /><br/>
and then we can write the derivative of <em>F(w)</em> as follows<br/>
<img src="http://i.imgur.com/oisE40X.png" alt="lossfunction gradient with delta" /><br/>
For commonly used loss functions <em>h(.)</em> it is easy to calculate derivative, but it is not clear how to obtain partial derivatives of <em>p</em> wrt <em>w</em>. <em>p</em> is the principle eigen vector of matrix <em>Q</em>. The above eigen vector equation can also be written as follows.<br/>
<img src="http://i.imgur.com/z00CXm4.png" alt="eigen vector reduced form." /><br/>
and taking the derivatives now gives<br/>
<img src="http://i.imgur.com/FhZVZEB.png" alt="derivative of p recursive form" /><br/>
above <em>p<sub>u</sub></em> and its partial derivative are entangled in the equation, however we compute the above values iteratively as follows<br/>
<img src="http://i.imgur.com/WneoOOn.png" alt="power method to compute p and its partial derivative iteratively." /><br/>
we initialize the vector <em>p</em> as <em>1/|V|</em> and all its derivatives as zeroes before the iteration starts and terminates the recursion till the <em>p</em> and its derivatives converge for an epsilon say <em>10e-12</em>.
To solve equation 4, we need partial derivative of <em>Q<sub>ju</sub></em>, this calculation is straight forward. When <em>(j,u) subset E</em> derivative of <em>Q<sub>ju</sub></em> is<br/>
<img src="http://i.imgur.com/aLDlWP4.png" alt="partial derivative of Qju" /><br/>
and derivative of <em>Q<sub>ju</sub></em> is zero if edge <em>(j,u)</em> is not a subset of <em>E</em>.</p>

<h2>My Implementation:</h2>

<p>We are given a huge network with existing connections. When predicting future link of a particular node, we consider that <em>s</em>, and the graph <em>G(E,V)</em> is
Here we explain how each helper function and main functions implements the above algorithm..</p>

<h3>FeaturesFromAdjacentMatrix.m:</h3>

<p>This is a temporary function specific to the facebook data that generates Features of each edge from a given adjacency matrix. For other problems this function must be replaced with something that generates feature vector for each edge based on graph <em>G(E,V)</em> and node, edge attributes. For an network with <em>n</em> nodes this function returns <em>n x n x m</em> matrix, where <em>m</em> is the size of parameter vector <em>w</em>(sometimes <em>m+1</em>)</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>Adjacency matrix, node attributes, edge attributes</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>psi size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>FeaturesToEdgeStrength.m:</h3>

<p>This function takes the feature matrix (<em>psi</em>) and the parameter vector (<em>w</em>) as arguments to return edge strength (<em>A</em>) and partial derivative of edge strength wrt to each parameter(<em>dA</em>). We also compute partial derivative of edge strength to make further calculations easier. We can vary edge strength function in future implementations, in this we used <em>sigmod(w x psi<sub>uv</sub>)</em> as edge strength function.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>psi size(nxnxm)</em></li>
<li><em>w size(1xm)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>dA size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>EdgeStrengthToTransitionProbability.m</h3>

<p>This function takes the edge strength matrix <em>A</em> and <em>alpha</em> to compute transition probability matrix <em>Q</em>.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>alpha size(1x1)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>Q size(nxn)</em></li>
</ul>
</li>
</ul>


<h3>EdgeStrengthToPartialdiffTransition.m</h3>

<p>This function computes partial derivative of transition probability matrix from <em>A</em>, <em>dA</em> and <em>alpha</em></p>

<ul>
<li><p>arguments:</p>

<ul>
<li><em>A size(nxn)</em></li>
<li><em>dA size(nxnxm)</em></li>
<li><em>alpha size(1x1)</em></li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li><em>dQ size(nxnxm)</em></li>
</ul>
</li>
</ul>


<h3>LossFunction.m</h3>

<p>This function takes as input parameters, adjacency matrix of the network, lambda and alpha.
* We get edge strength matrix and its partial derivatives from features and parameters
* We get transition probability and partial derivatives of it from <em>A</em> and <em>dA</em>
* We get stationary probabilities from <em>Q</em> and <em>dQ</em>
* Compute cost and gradient from the above variables, we can use various functions as loss function <em>h(.)</em>. Here we used wilcoxon loss function.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>param: parameters of the edge strength function, size(1,m)</li>
<li>features: features of all the edges in the network, size(n,n,m)</li>
<li>d: binary vector representing destination nodes, size(1,n)</li>
<li>lambda: regularization parameter, size(1,1)</li>
<li>alpha: random restart parameter, size(1,1)</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li>J: loss, size(1,1)</li>
<li>grad: gradient of cost wrt parameters, size(1,m)</li>
</ul>
</li>
</ul>


<h3>fmincg.m</h3>

<p>We use this function to do the minimization of the loss function, given a starting point for the parameters, and the function that computes loss and gradients for a given parameter vector. This is similar to fminunc function available in octave.</p>

<h3>GetNodesFromParam.m</h3>

<p>This function calculates the closest nodes to the root node given the parameters obtained after training.</p>

<ul>
<li><p>arguments:</p>

<ul>
<li>param: parameters, size(m,1)</li>
<li>features: feature matrix, size(n,n,m)</li>
<li>d: binary vector representing the destination nodes, size(1,n)</li>
<li>alpha: alpha value used in calculation of Q, size(1,1)</li>
<li>y: number of nodes to output</li>
</ul>
</li>
<li><p>returns:</p>

<ul>
<li>nodes: output nodes, size(1,y)</li>
<li>P: probabilities of the nodes, size(1,n)</li>
</ul>
</li>
</ul>


<h2>How to Train:</h2>

<p>Here I will show how to train the supervised random walk for a given root node <em>s</em> and edge features matrix <em>psi</em>.
I&rsquo;m not showing how to obtain the edge features. Given the network structure, node and edge attributes etc, you can experiment with different feature extraction techniques.
Here we have <em>psi</em></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>octave:1&gt; clear, close all, clc;
</span><span class='line'>octave:2&gt; rand("seed", 3410);
</span><span class='line'>octave:3&gt; m = size(psi)(3);
</span><span class='line'>octave:4&gt; n = size(psi)(1);
</span><span class='line'>octave:5&gt; initial_w = zeroes(1,m);   % initialize the parameters to zeros or rand
</span><span class='line'>octave:6&gt; initial_w = rand(1,n);
</span><span class='line'>
</span><span class='line'>% to calculate the loss for a given parameter vector.
</span><span class='line'>
</span><span class='line'>octave:7&gt; [loss, grad] = LossFunction(initial_w, psi, d, lambda=1, alpha=0.2, b=0.4);
</span><span class='line'>
</span><span class='line'>% d above is a binary vector that represents the destination nodes to begin with, 
</span><span class='line'>% you can initialize this randomly or obtain it from the graph
</span><span class='line'>
</span><span class='line'>% training
</span><span class='line'>octave:8&gt; options = optimset('GradObj', 'on', 'MaxIter', 20);
</span><span class='line'>octave:9&gt; [w,loss] = ...
</span><span class='line'>  fmincg(@(t)(LossFunction(t, psi, d, lambda=1,alpha=0.2,b=0.4)),
</span><span class='line'>  initial_w, options);
</span><span class='line'>
</span><span class='line'>% w obtained above is the parameters we obtained after gradient descent
</span><span class='line'>
</span><span class='line'>octave:10&gt; y = 10;
</span><span class='line'>octave:11&gt; [nodes, P] = GetNodesFromParam(w, psi,d,alpha = 0.2,y);</span></code></pre></td></tr></table></div></figure>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/10/welcome-to-my-website/">Welcome to My Blog</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-08-10T07:23:56+05:30'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>7:23 am</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Welcome!</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/08/02/miscellanious-notes/">Miscellanious Notes</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/07/30/elasticsearch-segmentation-moengage/">Elasticsearch, Segmentation, MoEngage</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/01/02/santander-product-recommendation-kaggle/">Santander Product Recommendation Kaggle</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/15/ghost-blog-as-your-github-profile-page/">Ghost Blog as Your Github User Page</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/09/13/stochastic-gradient-descent/">Stochastic Gradient Descent in AD.</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/syllogismos">@syllogismos</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'syllogismos',
            count: 5,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - syllogismos -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
